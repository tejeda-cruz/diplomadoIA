{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/husseinlopez/diplomadoIA/blob/main/M1-4_Ejercicios_Limpieza.ipynb)\n",
        "\n",
        "# M√≥dulo 1: Introducci√≥n a la Miner√≠a de Datos\n",
        "## Ejercicios Pr√°cticos de Limpieza y Preparaci√≥n de Datos\n",
        "\n",
        "**Diplomado en Inteligencia Artificial**  \n",
        "Dr. Irvin Hussein L√≥pez Nava\n",
        "CICESE - UABC\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objetivos de esta sesi√≥n\n",
        "\n",
        "1. **Identificar y corregir problemas de calidad** en conjuntos de datos reales\n",
        "2. **Manejar valores faltantes** con diferentes estrategias de imputaci√≥n\n",
        "3. **Detectar y tratar valores at√≠picos** sin perder informaci√≥n relevante\n",
        "4. **Aplicar t√©cnicas de reducci√≥n de dimensionalidad** (PCA, t-SNE)\n",
        "5. **Seleccionar atributos relevantes** mediante m√©todos Filter y Wrapper\n",
        "6. **Balancear clases desbalanceadas** con t√©cnicas de over/undersampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estructura del notebook\n",
        "\n",
        "### Parte 1: Limpieza de Datos\n",
        "* Inspecci√≥n inicial y detecci√≥n de problemas\n",
        "* Manejo de valores faltantes\n",
        "* Identificaci√≥n y tratamiento de outliers\n",
        "* Transformaciones y escalamiento\n",
        "\n",
        "### Parte 2: Reducci√≥n de Dimensionalidad\n",
        "* An√°lisis de Componentes Principales (PCA)\n",
        "* t-SNE para visualizaci√≥n no lineal\n",
        "* Comparaci√≥n de m√©todos\n",
        "\n",
        "### Parte 3: Selecci√≥n de Atributos\n",
        "* M√©todos basados en filtros (Filter)\n",
        "* M√©todos Wrapper\n",
        "* Consenso entre m√©todos\n",
        "\n",
        "### Parte 4: Balanceo de Clases\n",
        "* T√©cnicas de oversampling (SMOTE, ADASYN)\n",
        "* T√©cnicas de undersampling\n",
        "* Visualizaci√≥n del impacto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0. Configuraci√≥n del Entorno\n",
        "\n",
        "Importaremos todas las bibliotecas necesarias para el an√°lisis completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manejo de datos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Visualizaci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Configuraci√≥n de pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "# Reproducibilidad\n",
        "np.random.seed(42)\n",
        "\n",
        "# Ignorar warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úì Bibliotecas b√°sicas importadas correctamente\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocesamiento\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler, MinMaxScaler, RobustScaler,\n",
        "    LabelEncoder, OneHotEncoder, PowerTransformer\n",
        ")\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "# Reducci√≥n de dimensionalidad\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Selecci√≥n de atributos\n",
        "from sklearn.feature_selection import (\n",
        "    SelectKBest, chi2, f_classif, mutual_info_classif,\n",
        "    RFE\n",
        ")\n",
        "\n",
        "# Modelos para selecci√≥n\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Balanceo de clases\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "# Datasets\n",
        "from sklearn.datasets import load_breast_cancer, make_classification\n",
        "\n",
        "print(\"‚úì Bibliotecas de ML y preprocesamiento importadas correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Parte 1: Limpieza de Datos\n",
        "\n",
        "En esta secci√≥n trabajaremos con un dataset que presenta problemas comunes:\n",
        "- Valores faltantes\n",
        "- Valores at√≠picos\n",
        "- Escalas incompatibles\n",
        "- Tipos de datos incorrectos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Creaci√≥n de un Dataset con Problemas Reales\n",
        "\n",
        "Crearemos un dataset sint√©tico que simula datos m√©dicos con problemas t√≠picos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_messy_health_dataset(n_samples=500):\n",
        "    \"\"\"\n",
        "    Crea un dataset sint√©tico de datos de salud con problemas reales:\n",
        "    - Valores faltantes (MCAR, MAR, MNAR)\n",
        "    - Outliers\n",
        "    - Escalas inconsistentes\n",
        "    - Errores de registro\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Variables base\n",
        "    data = {\n",
        "        'edad': np.random.normal(45, 15, n_samples).clip(18, 90),\n",
        "        'peso': np.random.normal(70, 15, n_samples).clip(40, 150),\n",
        "        'estatura': np.random.normal(165, 10, n_samples).clip(140, 200),\n",
        "        'presion_sistolica': np.random.normal(120, 15, n_samples).clip(80, 200),\n",
        "        'presion_diastolica': np.random.normal(80, 10, n_samples).clip(60, 120),\n",
        "        'glucosa': np.random.normal(100, 20, n_samples).clip(70, 300),\n",
        "        'colesterol': np.random.normal(200, 40, n_samples).clip(120, 350),\n",
        "        'trigliceridos': np.random.normal(150, 50, n_samples).clip(50, 500),\n",
        "        'frecuencia_cardiaca': np.random.normal(75, 10, n_samples).clip(50, 120),\n",
        "    }\n",
        "    \n",
        "    df = pd.DataFrame(data)\n",
        "    \n",
        "    # Calcular IMC\n",
        "    df['imc'] = df['peso'] / ((df['estatura']/100) ** 2)\n",
        "    \n",
        "    # Variables categ√≥ricas\n",
        "    df['genero'] = np.random.choice(['M', 'F'], n_samples)\n",
        "    df['fumador'] = np.random.choice(['Si', 'No', 'Exfumador'], n_samples, p=[0.2, 0.6, 0.2])\n",
        "    df['diabetes'] = (df['glucosa'] > 126).astype(int)\n",
        "    df['hipertension'] = (df['presion_sistolica'] > 140).astype(int)\n",
        "    \n",
        "    # Introducir valores faltantes de diferentes tipos\n",
        "    \n",
        "    # MCAR (Missing Completely At Random) - 5% en edad\n",
        "    mcar_mask = np.random.random(n_samples) < 0.05\n",
        "    df.loc[mcar_mask, 'edad'] = np.nan\n",
        "    \n",
        "    # MAR (Missing At Random) - Personas con diabetes tienen m√°s faltantes en colesterol\n",
        "    mar_mask = (df['diabetes'] == 1) & (np.random.random(n_samples) < 0.15)\n",
        "    df.loc[mar_mask, 'colesterol'] = np.nan\n",
        "    \n",
        "    # MNAR (Missing Not At Random) - Valores altos de glucosa tienden a faltar m√°s\n",
        "    high_glucose = df['glucosa'] > df['glucosa'].quantile(0.75)\n",
        "    mnar_mask = high_glucose & (np.random.random(n_samples) < 0.10)\n",
        "    df.loc[mnar_mask, 'glucosa'] = np.nan\n",
        "    \n",
        "    # Valores faltantes adicionales\n",
        "    df.loc[np.random.random(n_samples) < 0.08, 'trigliceridos'] = np.nan\n",
        "    df.loc[np.random.random(n_samples) < 0.03, 'frecuencia_cardiaca'] = np.nan\n",
        "    \n",
        "    # Introducir outliers\n",
        "    \n",
        "    # Outliers extremos (errores de medici√≥n)\n",
        "    outlier_indices = np.random.choice(n_samples, size=10, replace=False)\n",
        "    df.loc[outlier_indices[:3], 'peso'] = np.random.uniform(200, 250, 3)\n",
        "    df.loc[outlier_indices[3:6], 'presion_sistolica'] = np.random.uniform(220, 280, 3)\n",
        "    df.loc[outlier_indices[6:], 'glucosa'] = np.random.uniform(400, 600, 4)\n",
        "    \n",
        "    # Outliers moderados (valores reales pero inusuales)\n",
        "    moderate_outliers = np.random.choice(n_samples, size=20, replace=False)\n",
        "    df.loc[moderate_outliers, 'colesterol'] = np.random.uniform(300, 400, 20)\n",
        "    \n",
        "    # Introducir inconsistencias\n",
        "    \n",
        "    # Algunas estatura en cm, otras (pocas) en metros\n",
        "    error_indices = np.random.choice(n_samples, size=5, replace=False)\n",
        "    df.loc[error_indices, 'estatura'] = df.loc[error_indices, 'estatura'] / 100\n",
        "    \n",
        "    # Calcular variable objetivo (riesgo cardiovascular)\n",
        "    risk_score = (\n",
        "        (df['edad'] > 55).astype(int) * 2 +\n",
        "        (df['imc'] > 30).astype(int) * 2 +\n",
        "        df['diabetes'] * 3 +\n",
        "        df['hipertension'] * 3 +\n",
        "        (df['fumador'] == 'Si').astype(int) * 2 +\n",
        "        (df['colesterol'] > 240).fillna(0).astype(int) * 2\n",
        "    )\n",
        "    \n",
        "    # Binarizar riesgo con algo de ruido\n",
        "    noise = np.random.random(n_samples) < 0.1\n",
        "    df['riesgo_alto'] = ((risk_score >= 6) != noise).astype(int)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Crear dataset\n",
        "df_health = create_messy_health_dataset(500)\n",
        "\n",
        "print(f\"Dataset creado con {len(df_health)} observaciones y {len(df_health.columns)} variables\")\n",
        "print(f\"\\nPrimeras filas:\")\n",
        "df_health.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Inspecci√≥n Inicial\n",
        "\n",
        "Primer vistazo a la estructura y calidad de los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_dataset(df):\n",
        "    \"\"\"\n",
        "    Realiza una inspecci√≥n completa del dataset\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"INSPECCI√ìN GENERAL DEL DATASET\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"\\nüìä Dimensiones: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
        "    print(f\"üíæ Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TIPOS DE DATOS\")\n",
        "    print(\"=\"*80)\n",
        "    print(df.dtypes)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"VALORES FALTANTES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = 100 * missing / len(df)\n",
        "    missing_table = pd.DataFrame({\n",
        "        'Columna': missing.index,\n",
        "        'Faltantes': missing.values,\n",
        "        'Porcentaje': missing_pct.values\n",
        "    })\n",
        "    missing_table = missing_table[missing_table['Faltantes'] > 0].sort_values('Porcentaje', ascending=False)\n",
        "    \n",
        "    if len(missing_table) > 0:\n",
        "        print(missing_table.to_string(index=False))\n",
        "        print(f\"\\n‚ö†Ô∏è  Total de valores faltantes: {missing.sum()} ({100*missing.sum()/(df.shape[0]*df.shape[1]):.2f}% del dataset)\")\n",
        "    else:\n",
        "        print(\"‚úì No hay valores faltantes\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ESTAD√çSTICAS DESCRIPTIVAS (VARIABLES NUM√âRICAS)\")\n",
        "    print(\"=\"*80)\n",
        "    print(df.describe().T)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DISTRIBUCI√ìN DE VARIABLES CATEG√ìRICAS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "    for col in categorical_cols:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(df[col].value_counts())\n",
        "        print(f\"Valores √∫nicos: {df[col].nunique()}\")\n",
        "\n",
        "inspect_dataset(df_health)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Visualizaci√≥n de Valores Faltantes\n",
        "\n",
        "Entender el patr√≥n de datos faltantes es crucial para decidir c√≥mo manejarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_missing_data(df):\n",
        "    \"\"\"\n",
        "    Crea visualizaciones comprehensivas de valores faltantes\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # 1. Matriz de valores faltantes\n",
        "    ax1 = fig.add_subplot(gs[0, :])\n",
        "    missing_matrix = df.isnull().astype(int)\n",
        "    sns.heatmap(missing_matrix.T, cmap='YlOrRd', cbar=True, ax=ax1,\n",
        "                yticklabels=df.columns, xticklabels=False)\n",
        "    ax1.set_title('Matriz de Valores Faltantes\\n(Amarillo = Presente, Rojo = Faltante)', \n",
        "                  fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Observaciones')\n",
        "    \n",
        "    # 2. Porcentaje de valores faltantes por columna\n",
        "    ax2 = fig.add_subplot(gs[1, 0])\n",
        "    missing_pct = 100 * df.isnull().sum() / len(df)\n",
        "    missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=True)\n",
        "    \n",
        "    if len(missing_pct) > 0:\n",
        "        colors = ['#d62728' if x > 10 else '#ff7f0e' if x > 5 else '#2ca02c' for x in missing_pct]\n",
        "        missing_pct.plot(kind='barh', ax=ax2, color=colors)\n",
        "        ax2.set_xlabel('Porcentaje de valores faltantes (%)')\n",
        "        ax2.set_title('Valores Faltantes por Variable', fontweight='bold')\n",
        "        ax2.axvline(x=5, color='orange', linestyle='--', alpha=0.5, label='5%')\n",
        "        ax2.axvline(x=10, color='red', linestyle='--', alpha=0.5, label='10%')\n",
        "        ax2.legend()\n",
        "        ax2.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # 3. N√∫mero de valores faltantes por fila\n",
        "    ax3 = fig.add_subplot(gs[1, 1])\n",
        "    missing_per_row = df.isnull().sum(axis=1)\n",
        "    missing_counts = missing_per_row.value_counts().sort_index()\n",
        "    \n",
        "    ax3.bar(missing_counts.index, missing_counts.values, color='steelblue', alpha=0.7)\n",
        "    ax3.set_xlabel('N√∫mero de valores faltantes')\n",
        "    ax3.set_ylabel('N√∫mero de observaciones')\n",
        "    ax3.set_title('Distribuci√≥n de Valores Faltantes por Fila', fontweight='bold')\n",
        "    ax3.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # A√±adir texto con estad√≠sticas\n",
        "    total_rows_with_missing = (missing_per_row > 0).sum()\n",
        "    ax3.text(0.95, 0.95, \n",
        "             f'Filas con faltantes: {total_rows_with_missing}\\n'\n",
        "             f'Filas completas: {len(df) - total_rows_with_missing}',\n",
        "             transform=ax3.transAxes, fontsize=10,\n",
        "             verticalalignment='top', horizontalalignment='right',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    # 4. Correlaci√≥n entre valores faltantes\n",
        "    ax4 = fig.add_subplot(gs[2, :])\n",
        "    missing_corr = df.isnull().corr()\n",
        "    mask = np.triu(np.ones_like(missing_corr), k=1)\n",
        "    \n",
        "    sns.heatmap(missing_corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "                center=0, ax=ax4, cbar_kws={'label': 'Correlaci√≥n'})\n",
        "    ax4.set_title('Correlaci√≥n entre Patrones de Valores Faltantes\\n'\n",
        "                  '(Valores altos sugieren faltantes no aleatorios)', fontweight='bold')\n",
        "    \n",
        "    plt.suptitle('An√°lisis Comprehensivo de Valores Faltantes', \n",
        "                 fontsize=16, fontweight='bold', y=0.995)\n",
        "    \n",
        "    return fig\n",
        "\n",
        "fig = visualize_missing_data(df_health)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 An√°lisis de Patrones de Valores Faltantes\n",
        "\n",
        "Determinar si los valores faltantes son MCAR, MAR o MNAR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis detallado de patrones de valores faltantes\n",
        "def analyze_missing_patterns(df):\n",
        "    \"\"\"\n",
        "    Analiza si los valores faltantes son MCAR, MAR o MNAR\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"AN√ÅLISIS DE PATRONES DE VALORES FALTANTES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Crear indicadores de faltantes\n",
        "    cols_with_missing = df.columns[df.isnull().any()].tolist()\n",
        "    \n",
        "    for col in cols_with_missing:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Variable: {col}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        missing_mask = df[col].isnull()\n",
        "        \n",
        "        # Comparar caracter√≠sticas entre observaciones con y sin faltantes\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        numeric_cols = [c for c in numeric_cols if c != col]\n",
        "        \n",
        "        print(\"\\nComparaci√≥n de medias (con faltantes vs sin faltantes):\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        for other_col in numeric_cols[:5]:  # Limitamos a 5 para no saturar\n",
        "            if df[other_col].notna().sum() > 0:\n",
        "                mean_missing = df.loc[missing_mask, other_col].mean()\n",
        "                mean_present = df.loc[~missing_mask, other_col].mean()\n",
        "                \n",
        "                if pd.notna(mean_missing) and pd.notna(mean_present):\n",
        "                    diff_pct = 100 * (mean_missing - mean_present) / mean_present\n",
        "                    \n",
        "                    # Test t para diferencia de medias\n",
        "                    try:\n",
        "                        t_stat, p_value = stats.ttest_ind(\n",
        "                            df.loc[missing_mask, other_col].dropna(),\n",
        "                            df.loc[~missing_mask, other_col].dropna()\n",
        "                        )\n",
        "                        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
        "                    except:\n",
        "                        p_value = np.nan\n",
        "                        significance = \"\"\n",
        "                    \n",
        "                    print(f\"{other_col:30s}: {mean_present:7.2f} ‚Üí {mean_missing:7.2f} \"\n",
        "                          f\"({diff_pct:+6.1f}%) p={p_value:.3f} {significance}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INTERPRETACI√ìN:\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"* = p < 0.05  (diferencia estad√≠sticamente significativa)\")\n",
        "    print(\"** = p < 0.01 (alta significancia)\")\n",
        "    print(\"*** = p < 0.001 (muy alta significancia)\")\n",
        "    print(\"\\nDiferencias significativas sugieren valores faltantes MAR o MNAR\")\n",
        "    print(\"No diferencias sugiere MCAR (Missing Completely At Random)\")\n",
        "\n",
        "analyze_missing_patterns(df_health)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Manejo de Valores Faltantes\n",
        "\n",
        "Compararemos diferentes estrategias de imputaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_imputation_methods(df, column):\n",
        "    \"\"\"\n",
        "    Compara diferentes m√©todos de imputaci√≥n en una columna espec√≠fica\n",
        "    \"\"\"\n",
        "    df_test = df.copy()\n",
        "    missing_mask = df_test[column].isnull()\n",
        "    original_values = df_test.loc[~missing_mask, column].copy()\n",
        "    \n",
        "    methods = {}\n",
        "    \n",
        "    # 1. Eliminaci√≥n\n",
        "    methods['Eliminaci√≥n'] = df_test[column].dropna()\n",
        "    \n",
        "    # 2. Media\n",
        "    imputer_mean = SimpleImputer(strategy='mean')\n",
        "    methods['Media'] = pd.Series(\n",
        "        imputer_mean.fit_transform(df_test[[column]]).ravel(),\n",
        "        index=df_test.index\n",
        "    )\n",
        "    \n",
        "    # 3. Mediana\n",
        "    imputer_median = SimpleImputer(strategy='median')\n",
        "    methods['Mediana'] = pd.Series(\n",
        "        imputer_median.fit_transform(df_test[[column]]).ravel(),\n",
        "        index=df_test.index\n",
        "    )\n",
        "    \n",
        "    # 4. KNN Imputer\n",
        "    numeric_cols = df_test.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if len(numeric_cols) > 1:\n",
        "        imputer_knn = KNNImputer(n_neighbors=5)\n",
        "        df_knn = df_test[numeric_cols].copy()\n",
        "        imputed_knn = imputer_knn.fit_transform(df_knn)\n",
        "        col_idx = numeric_cols.index(column)\n",
        "        methods['KNN (k=5)'] = pd.Series(\n",
        "            imputed_knn[:, col_idx],\n",
        "            index=df_test.index\n",
        "        )\n",
        "    \n",
        "    # Visualizaci√≥n comparativa\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    # Plot original\n",
        "    ax = axes[0]\n",
        "    ax.hist(original_values, bins=30, alpha=0.7, color='gray', edgecolor='black')\n",
        "    ax.axvline(original_values.mean(), color='red', linestyle='--', \n",
        "               linewidth=2, label=f'Media: {original_values.mean():.2f}')\n",
        "    ax.axvline(original_values.median(), color='blue', linestyle='--', \n",
        "               linewidth=2, label=f'Mediana: {original_values.median():.2f}')\n",
        "    ax.set_title('Distribuci√≥n Original\\\\n(sin valores faltantes)', fontweight='bold')\n",
        "    ax.set_xlabel(column)\n",
        "    ax.set_ylabel('Frecuencia')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "    \n",
        "    # Plot cada m√©todo\n",
        "    for idx, (method_name, imputed_data) in enumerate(methods.items(), 1):\n",
        "        if idx >= len(axes):\n",
        "            break\n",
        "        ax = axes[idx]\n",
        "        ax.hist(original_values, bins=30, alpha=0.4, color='gray', label='Original', edgecolor='black')\n",
        "        ax.hist(imputed_data.dropna(), bins=30, alpha=0.6, color='steelblue', label=method_name, edgecolor='black')\n",
        "        mean_diff = imputed_data.mean() - original_values.mean()\n",
        "        std_diff = imputed_data.std() - original_values.std()\n",
        "        ax.set_title(f'{method_name}\\\\nŒîmedia: {mean_diff:+.2f}, Œîstd: {std_diff:+.2f}', fontweight='bold')\n",
        "        ax.set_xlabel(column)\n",
        "        ax.set_ylabel('Frecuencia')\n",
        "        ax.legend()\n",
        "        ax.grid(alpha=0.3)\n",
        "    \n",
        "    for idx in range(len(methods) + 1, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'Comparaci√≥n de M√©todos de Imputaci√≥n: {column}', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Estad√≠sticas\n",
        "    print(\"=\"*80)\n",
        "    print(f\"COMPARACI√ìN DE M√âTODOS DE IMPUTACI√ìN: {column}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\\\nOriginal: N={len(original_values)}, Media={original_values.mean():.2f}, Std={original_values.std():.2f}\")\n",
        "    for method_name, imputed_data in methods.items():\n",
        "        print(f\"{method_name}: N={len(imputed_data.dropna())}, Media={imputed_data.mean():.2f}, Std={imputed_data.std():.2f}\")\n",
        "    \n",
        "    return fig, methods\n",
        "\n",
        "# Comparar m√©todos para glucosa\n",
        "fig, methods = compare_imputation_methods(df_health, 'glucosa')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplicar imputaci√≥n con KNN\n",
        "def apply_imputation(df, strategy='knn'):\n",
        "    \"\"\"\n",
        "    Aplica estrategia de imputaci√≥n al dataset completo\n",
        "    \"\"\"\n",
        "    df_imputed = df.copy()\n",
        "    \n",
        "    numeric_cols = df_imputed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = df_imputed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    \n",
        "    if strategy == 'knn':\n",
        "        imputer_num = KNNImputer(n_neighbors=5)\n",
        "        df_imputed[numeric_cols] = imputer_num.fit_transform(df_imputed[numeric_cols])\n",
        "        \n",
        "        for col in categorical_cols:\n",
        "            if df_imputed[col].isnull().any():\n",
        "                mode_value = df_imputed[col].mode()[0]\n",
        "                df_imputed[col].fillna(mode_value, inplace=True)\n",
        "    \n",
        "    print(f\"Imputaci√≥n aplicada con estrategia: {strategy}\")\n",
        "    print(f\"Filas antes: {len(df)} ‚Üí Filas despu√©s: {len(df_imputed)}\")\n",
        "    print(f\"Valores faltantes restantes: {df_imputed.isnull().sum().sum()}\")\n",
        "    \n",
        "    return df_imputed\n",
        "\n",
        "df_health_imputed = apply_imputation(df_health, strategy='knn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Detecci√≥n y Tratamiento de Outliers\n",
        "\n",
        "Identificaremos valores at√≠picos usando m√∫ltiples m√©todos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_outliers_multiple_methods(df, column):\n",
        "    \"\"\"\n",
        "    Detecta outliers usando diferentes m√©todos:\n",
        "    1. IQR (Interquartile Range)\n",
        "    2. Z-score\n",
        "    3. Isolation Forest\n",
        "    \"\"\"\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "    \n",
        "    # Trabajar solo con datos no nulos\n",
        "    data = df[column].dropna().values.reshape(-1, 1)\n",
        "    indices_validos = df[column].dropna().index  # √çndices de datos no nulos\n",
        "    \n",
        "    outliers = {}\n",
        "    \n",
        "    # 1. M√©todo IQR\n",
        "    Q1 = np.percentile(data, 25)\n",
        "    Q3 = np.percentile(data, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers['IQR'] = (data < lower_bound) | (data > upper_bound)\n",
        "    \n",
        "    # 2. Z-score\n",
        "    z_scores = np.abs(stats.zscore(data))\n",
        "    outliers['Z-score'] = z_scores > 3\n",
        "    \n",
        "    # 3. Isolation Forest\n",
        "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "    outliers['Isolation Forest'] = iso_forest.fit_predict(data) == -1\n",
        "    \n",
        "    # Visualizaci√≥n\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Box plot\n",
        "    ax = axes[0, 0]\n",
        "    bp = ax.boxplot([data.ravel()], vert=True, patch_artist=True,\n",
        "                     boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
        "                     medianprops=dict(color='red', linewidth=2))\n",
        "    ax.axhline(lower_bound, color='orange', linestyle='--', label=f'IQR lower: {lower_bound:.2f}')\n",
        "    ax.axhline(upper_bound, color='orange', linestyle='--', label=f'IQR upper: {upper_bound:.2f}')\n",
        "    ax.set_ylabel(column)\n",
        "    ax.set_title('Box Plot con L√≠mites IQR', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "    \n",
        "    # Distribuci√≥n con outliers\n",
        "    ax = axes[0, 1]\n",
        "    ax.hist(data, bins=50, alpha=0.6, color='steelblue', edgecolor='black')\n",
        "    for method_name, is_outlier in outliers.items():\n",
        "        outlier_values = data[is_outlier.ravel()]\n",
        "        if len(outlier_values) > 0:\n",
        "            ax.scatter(outlier_values, [0] * len(outlier_values), s=100, alpha=0.6, label=method_name)\n",
        "    ax.set_xlabel(column)\n",
        "    ax.set_ylabel('Frecuencia')\n",
        "    ax.set_title('Distribuci√≥n con Outliers Detectados', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "    \n",
        "    # Z-scores\n",
        "    ax = axes[1, 0]\n",
        "    sorted_idx = np.argsort(data.ravel())\n",
        "    ax.scatter(range(len(data)), z_scores[sorted_idx], alpha=0.5, s=20)\n",
        "    ax.axhline(3, color='red', linestyle='--', label='Umbral Z=3')\n",
        "    ax.set_xlabel('Observaciones (ordenadas)')\n",
        "    ax.set_ylabel('|Z-score|')\n",
        "    ax.set_title('Z-scores', fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "    \n",
        "    # Comparaci√≥n\n",
        "    ax = axes[1, 1]\n",
        "    method_names = list(outliers.keys())\n",
        "    counts = [outliers[m].sum() for m in method_names]\n",
        "    bars = ax.barh(method_names, counts, color=['#ff7f0e', '#2ca02c', '#d62728'])\n",
        "    ax.set_xlabel('N√∫mero de outliers detectados')\n",
        "    ax.set_title('Comparaci√≥n de M√©todos', fontweight='bold')\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    for bar, count in zip(bars, counts):\n",
        "        width = bar.get_width()\n",
        "        ax.text(width, bar.get_y() + bar.get_height()/2,\n",
        "               f'{int(count)} ({100*count/len(data):.1f}%)',\n",
        "               ha='left', va='center', fontweight='bold')\n",
        "    \n",
        "    plt.suptitle(f'Detecci√≥n de Outliers: {column}', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Consenso (al menos 2 m√©todos coinciden)\n",
        "    # Sumar arrays booleanos: cada True cuenta como 1\n",
        "    consensus_count = np.zeros(len(data), dtype=int)\n",
        "    for method_array in outliers.values():\n",
        "        consensus_count += method_array.ravel().astype(int)\n",
        "    \n",
        "    # Outliers detectados por al menos 2 m√©todos\n",
        "    consensus_outliers_indices = consensus_count >= 2\n",
        "    \n",
        "    # Crear m√°scara booleana del tama√±o completo del DataFrame\n",
        "    consensus_mask = pd.Series(False, index=df.index, dtype=bool)\n",
        "    \n",
        "    # Marcar como True los √≠ndices que son outliers\n",
        "    outlier_positions = indices_validos[consensus_outliers_indices]\n",
        "    consensus_mask.loc[outlier_positions] = True\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(f\"DETECCI√ìN DE OUTLIERS: {column}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nTotal observaciones v√°lidas: {len(data)}\")\n",
        "    for method, is_outlier in outliers.items():\n",
        "        n_outliers = is_outlier.sum()\n",
        "        print(f\"{method:20s}: {n_outliers:4d} ({100*n_outliers/len(data):5.2f}%)\")\n",
        "    print(f\"\\nConsenso (‚â•2 m√©todos): {consensus_outliers_indices.sum()} ({100*consensus_outliers_indices.sum()/len(data):.2f}%)\")\n",
        "    print(f\"M√°scara creada con {len(consensus_mask)} elementos, {consensus_mask.sum()} marcados como outliers\")\n",
        "    \n",
        "    return fig, outliers, consensus_mask\n",
        "\n",
        "# Detectar outliers\n",
        "fig_out, outliers_peso, consensus_peso = detect_outliers_multiple_methods(df_health_imputed, 'peso')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tratamiento de outliers\n",
        "def treat_outliers(df, column, method='cap', outlier_mask=None):\n",
        "    \"\"\"\n",
        "    Trata outliers usando diferentes estrategias\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        DataFrame con los datos\n",
        "    column : str\n",
        "        Nombre de la columna a tratar\n",
        "    method : str\n",
        "        M√©todo de tratamiento: 'remove', 'cap', 'transform'\n",
        "    outlier_mask : Series booleana\n",
        "        M√°scara indicando outliers (mismo √≠ndice que df)\n",
        "    \"\"\"\n",
        "    df_treated = df.copy()\n",
        "    original = df_treated[column].copy()\n",
        "    \n",
        "    if outlier_mask is None:\n",
        "        # Si no hay m√°scara, usar IQR\n",
        "        Q1 = df_treated[column].quantile(0.25)\n",
        "        Q3 = df_treated[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outlier_mask = (df_treated[column] < Q1 - 1.5*IQR) | (df_treated[column] > Q3 + 1.5*IQR)\n",
        "    \n",
        "    # Verificar que la m√°scara tenga el tama√±o correcto\n",
        "    if len(outlier_mask) != len(df_treated):\n",
        "        raise ValueError(f\"outlier_mask tiene tama√±o {len(outlier_mask)} pero DataFrame tiene {len(df_treated)} filas\")\n",
        "    \n",
        "    n_outliers = outlier_mask.sum()\n",
        "    print(f\"Tratando {n_outliers} outliers en '{column}' usando m√©todo '{method}'\")\n",
        "    \n",
        "    if method == 'remove':\n",
        "        df_treated = df_treated[~outlier_mask]\n",
        "        print(f\"  ‚Üí Filas eliminadas: {n_outliers}\")\n",
        "        print(f\"  ‚Üí Filas restantes: {len(df_treated)}\")\n",
        "    elif method == 'cap':\n",
        "        lower = df_treated[column].quantile(0.05)\n",
        "        upper = df_treated[column].quantile(0.95)\n",
        "        df_treated[column] = df_treated[column].clip(lower, upper)\n",
        "        print(f\"  ‚Üí Valores limitados a [{lower:.2f}, {upper:.2f}]\")\n",
        "    elif method == 'transform':\n",
        "        # Winsorizaci√≥n: reemplazar outliers con valores percentiles\n",
        "        lower = df_treated[column].quantile(0.05)\n",
        "        upper = df_treated[column].quantile(0.95)\n",
        "        df_treated.loc[df_treated[column] < lower, column] = lower\n",
        "        df_treated.loc[df_treated[column] > upper, column] = upper\n",
        "        print(f\"  ‚Üí Outliers reemplazados con percentiles 5 y 95\")\n",
        "    \n",
        "    # Visualizaci√≥n\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    # Antes\n",
        "    axes[0].hist(original.dropna(), bins=50, alpha=0.7, color='red', edgecolor='black')\n",
        "    axes[0].set_title('Antes del Tratamiento', fontweight='bold')\n",
        "    axes[0].set_xlabel(column)\n",
        "    axes[0].set_ylabel('Frecuencia')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Despu√©s\n",
        "    axes[1].hist(df_treated[column].dropna(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
        "    axes[1].set_title(f'Despu√©s ({method})', fontweight='bold')\n",
        "    axes[1].set_xlabel(column)\n",
        "    axes[1].set_ylabel('Frecuencia')\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    # Comparaci√≥n con boxplots\n",
        "    if method != 'remove':\n",
        "        axes[2].boxplot([original.dropna(), df_treated[column].dropna()],\n",
        "                       labels=['Antes', 'Despu√©s'], patch_artist=True)\n",
        "    else:\n",
        "        # Para remove, solo mostrar el despu√©s\n",
        "        axes[2].boxplot([original.dropna(), df_treated[column].dropna()],\n",
        "                       labels=['Antes\\n(todos)', 'Despu√©s\\n(sin outliers)'], patch_artist=True)\n",
        "    axes[2].set_title('Comparaci√≥n', fontweight='bold')\n",
        "    axes[2].set_ylabel(column)\n",
        "    axes[2].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    return df_treated, fig\n",
        "\n",
        "# Probar diferentes m√©todos\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EJEMPLO 1: M√©todo 'cap' (limitar valores)\")\n",
        "print(\"=\"*80)\n",
        "df_peso_capped, fig_cap = treat_outliers(df_health_imputed, 'peso', method='cap', outlier_mask=consensus_peso)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EJEMPLO 2: M√©todo 'remove' (eliminar filas)\")\n",
        "print(\"=\"*80)\n",
        "df_peso_removed, fig_remove = treat_outliers(df_health_imputed, 'peso', method='remove', outlier_mask=consensus_peso)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Escalamiento y Transformaciones\n",
        "\n",
        "Comparaci√≥n de diferentes m√©todos de escalamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_scaling_methods(df, columns=None):\n",
        "    \"\"\"\n",
        "    Compara diferentes m√©todos de escalamiento\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = df.select_dtypes(include=[np.number]).columns[:4]\n",
        "    \n",
        "    df_subset = df[columns].copy()\n",
        "    \n",
        "    scalers = {\n",
        "        'Original': None,\n",
        "        'StandardScaler': StandardScaler(),\n",
        "        'MinMaxScaler': MinMaxScaler(),\n",
        "        'RobustScaler': RobustScaler(),\n",
        "        'PowerTransformer': PowerTransformer(method='yeo-johnson')\n",
        "    }\n",
        "    \n",
        "    scaled_data = {}\n",
        "    for name, scaler in scalers.items():\n",
        "        if scaler is None:\n",
        "            scaled_data[name] = df_subset.values\n",
        "        else:\n",
        "            scaled_data[name] = scaler.fit_transform(df_subset)\n",
        "    \n",
        "    # Visualizaci√≥n\n",
        "    fig, axes = plt.subplots(len(scalers), len(columns), figsize=(5*len(columns), 4*len(scalers)))\n",
        "    if len(columns) == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "    \n",
        "    for i, (method_name, data) in enumerate(scaled_data.items()):\n",
        "        for j, col in enumerate(columns):\n",
        "            ax = axes[i, j]\n",
        "            ax.hist(data[:, j], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "            mean = np.mean(data[:, j])\n",
        "            std = np.std(data[:, j])\n",
        "            if i == 0:\n",
        "                ax.set_title(f'{col}\\\\n{method_name}\\\\nŒº={mean:.2f}, œÉ={std:.2f}', fontweight='bold')\n",
        "            else:\n",
        "                ax.set_title(f'{method_name}\\\\nŒº={mean:.2f}, œÉ={std:.2f}', fontweight='bold')\n",
        "            ax.axvline(mean, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
        "            ax.grid(alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('Comparaci√≥n de M√©todos de Escalamiento', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    return fig, scaled_data\n",
        "\n",
        "cols_to_scale = ['edad', 'peso', 'presion_sistolica', 'glucosa']\n",
        "fig_scale, scaled_results = compare_scaling_methods(df_health_imputed, cols_to_scale)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Parte 2: Reducci√≥n de Dimensionalidad\n",
        "\n",
        "Exploraremos t√©cnicas para reducir el n√∫mero de variables preservando la mayor cantidad de informaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Preparaci√≥n: Dataset de C√°ncer de Mama\n",
        "\n",
        "Usaremos el dataset cl√°sico de Wisconsin Breast Cancer con 30 caracter√≠sticas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATASET: Wisconsin Breast Cancer\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nDimensiones: {X_cancer.shape}\")\n",
        "print(f\"Clases: {np.unique(y_cancer, return_counts=True)}\")\n",
        "print(f\"\\nPrimeras caracter√≠sticas:\")\n",
        "print(X_cancer.columns.tolist()[:10])\n",
        "print(\"...\")\n",
        "\n",
        "# Escalamiento previo (necesario para PCA y t-SNE)\n",
        "scaler = StandardScaler()\n",
        "X_cancer_scaled = scaler.fit_transform(X_cancer)\n",
        "X_cancer_scaled_df = pd.DataFrame(X_cancer_scaled, columns=X_cancer.columns)\n",
        "\n",
        "print(f\"\\n‚úì Datos escalados con StandardScaler\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Parte 3: Selecci√≥n de Atributos\n",
        "\n",
        "Identificaremos las caracter√≠sticas m√°s relevantes usando m√©todos Filter y Wrapper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 M√©todos Filter\n",
        "\n",
        "Eval√∫an la relevancia de cada atributo independientemente del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 An√°lisis de Componentes Principales (PCA)\n",
        "\n",
        "PCA encuentra direcciones ortogonales de m√°xima varianza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_pca_analysis(X, y=None, feature_names=None):\n",
        "    \"\"\"\n",
        "    Realiza an√°lisis completo de PCA con m√∫ltiples visualizaciones\n",
        "    \"\"\"\n",
        "    # PCA completo\n",
        "    pca_full = PCA()\n",
        "    X_pca_full = pca_full.fit_transform(X)\n",
        "    \n",
        "    explained_variance = pca_full.explained_variance_ratio_\n",
        "    cumulative_variance = np.cumsum(explained_variance)\n",
        "    \n",
        "    # Encontrar componentes para 90%, 95%, 99%\n",
        "    n_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
        "    n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "    n_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"AN√ÅLISIS PCA\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nDimensiones originales: {X.shape[1]}\")\n",
        "    print(f\"\\nComponentes necesarios para:\")\n",
        "    print(f\"  - 90% varianza: {n_90} componentes\")\n",
        "    print(f\"  - 95% varianza: {n_95} componentes\")\n",
        "    print(f\"  - 99% varianza: {n_99} componentes\")\n",
        "    print(f\"\\nPrimeros 5 componentes explican: {cumulative_variance[4]:.1%}\")\n",
        "    print(f\"Primeros 10 componentes explican: {cumulative_variance[9]:.1%}\")\n",
        "    \n",
        "    # Visualizaciones\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # 1. Varianza por componente (Scree Plot)\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    components = np.arange(1, min(21, len(explained_variance)+1))\n",
        "    ax1.bar(components, explained_variance[:20], alpha=0.7, color='steelblue', edgecolor='black')\n",
        "    ax1.set_xlabel('Componente Principal', fontsize=12)\n",
        "    ax1.set_ylabel('Varianza Explicada', fontsize=12)\n",
        "    ax1.set_title('Scree Plot\\n(Primeras 20 componentes)', fontweight='bold', fontsize=13)\n",
        "    ax1.grid(alpha=0.3, axis='y')\n",
        "    ax1.set_xticks(components[::2])\n",
        "    \n",
        "    # 2. Varianza acumulada\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax2.plot(range(1, len(cumulative_variance)+1), cumulative_variance, \n",
        "             marker='o', linewidth=2, markersize=4, color='steelblue')\n",
        "    ax2.axhline(y=0.90, color='green', linestyle='--', linewidth=2, label='90%', alpha=0.7)\n",
        "    ax2.axhline(y=0.95, color='orange', linestyle='--', linewidth=2, label='95%', alpha=0.7)\n",
        "    ax2.axhline(y=0.99, color='red', linestyle='--', linewidth=2, label='99%', alpha=0.7)\n",
        "    ax2.axvline(x=n_95, color='orange', linestyle=':', alpha=0.5)\n",
        "    ax2.set_xlabel('N√∫mero de Componentes', fontsize=12)\n",
        "    ax2.set_ylabel('Varianza Acumulada', fontsize=12)\n",
        "    ax2.set_title('Varianza Explicada Acumulada', fontweight='bold', fontsize=13)\n",
        "    ax2.legend(fontsize=10)\n",
        "    ax2.grid(alpha=0.3)\n",
        "    ax2.set_xlim(0, min(30, len(cumulative_variance)))\n",
        "    \n",
        "    # 3. Raz√≥n de varianza (Kaiser criterion)\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    eigenvalues = pca_full.explained_variance_[:20]\n",
        "    ax3.plot(range(1, len(eigenvalues)+1), eigenvalues, marker='s', \n",
        "             linewidth=2, markersize=6, color='darkred')\n",
        "    ax3.axhline(y=1, color='black', linestyle='--', linewidth=2, label='Kaiser criterion (Œª=1)', alpha=0.7)\n",
        "    ax3.set_xlabel('Componente Principal', fontsize=12)\n",
        "    ax3.set_ylabel('Eigenvalue (Œª)', fontsize=12)\n",
        "    ax3.set_title('Eigenvalues\\n(Kaiser: retener Œª > 1)', fontweight='bold', fontsize=13)\n",
        "    ax3.legend(fontsize=10)\n",
        "    ax3.grid(alpha=0.3)\n",
        "    n_kaiser = np.sum(pca_full.explained_variance_ > 1)\n",
        "    ax3.text(0.98, 0.98, f'n={n_kaiser}', transform=ax3.transAxes,\n",
        "             ha='right', va='top', fontsize=11, fontweight='bold',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "    \n",
        "    # 4. Proyecci√≥n 2D (PC1 vs PC2)\n",
        "    ax4 = fig.add_subplot(gs[1, :2])\n",
        "    if y is not None:\n",
        "        scatter = ax4.scatter(X_pca_full[:, 0], X_pca_full[:, 1], \n",
        "                            c=y, cmap='RdYlGn', alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "        cbar = plt.colorbar(scatter, ax=ax4)\n",
        "        cbar.set_label('Clase', fontsize=11)\n",
        "    else:\n",
        "        ax4.scatter(X_pca_full[:, 0], X_pca_full[:, 1], \n",
        "                   alpha=0.6, s=50, color='steelblue', edgecolors='black', linewidth=0.5)\n",
        "    \n",
        "    ax4.set_xlabel(f'PC1 ({explained_variance[0]:.1%} varianza)', fontsize=12)\n",
        "    ax4.set_ylabel(f'PC2 ({explained_variance[1]:.1%} varianza)', fontsize=12)\n",
        "    ax4.set_title(f'Proyecci√≥n en Primeras 2 Componentes\\n(Total: {explained_variance[0]+explained_variance[1]:.1%} varianza)', \n",
        "                 fontweight='bold', fontsize=13)\n",
        "    ax4.grid(alpha=0.3)\n",
        "    ax4.axhline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "    ax4.axvline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "    \n",
        "    # 5. Loadings PC1\n",
        "    ax5 = fig.add_subplot(gs[1, 2])\n",
        "    if feature_names is not None:\n",
        "        loadings_pc1 = pd.Series(pca_full.components_[0], index=feature_names)\n",
        "        top_loadings = pd.concat([loadings_pc1.nlargest(5), loadings_pc1.nsmallest(5)])\n",
        "        colors = ['red' if x < 0 else 'green' for x in top_loadings.values]\n",
        "        top_loadings.plot(kind='barh', ax=ax5, color=colors, alpha=0.7, edgecolor='black')\n",
        "        ax5.set_xlabel('Loading', fontsize=11)\n",
        "        ax5.set_title('Top Loadings PC1', fontweight='bold', fontsize=13)\n",
        "        ax5.axvline(0, color='black', linewidth=1)\n",
        "        ax5.grid(alpha=0.3, axis='x')\n",
        "    \n",
        "    # 6. Loadings PC2\n",
        "    ax6 = fig.add_subplot(gs[2, 0])\n",
        "    if feature_names is not None:\n",
        "        loadings_pc2 = pd.Series(pca_full.components_[1], index=feature_names)\n",
        "        top_loadings = pd.concat([loadings_pc2.nlargest(5), loadings_pc2.nsmallest(5)])\n",
        "        colors = ['red' if x < 0 else 'green' for x in top_loadings.values]\n",
        "        top_loadings.plot(kind='barh', ax=ax6, color=colors, alpha=0.7, edgecolor='black')\n",
        "        ax6.set_xlabel('Loading', fontsize=11)\n",
        "        ax6.set_title('Top Loadings PC2', fontweight='bold', fontsize=13)\n",
        "        ax6.axvline(0, color='black', linewidth=1)\n",
        "        ax6.grid(alpha=0.3, axis='x')\n",
        "    \n",
        "    # 7. Biplot (PC1 vs PC2 con vectores)\n",
        "    ax7 = fig.add_subplot(gs[2, 1:])\n",
        "    if y is not None:\n",
        "        scatter = ax7.scatter(X_pca_full[:, 0], X_pca_full[:, 1], \n",
        "                            c=y, cmap='RdYlGn', alpha=0.3, s=30)\n",
        "    else:\n",
        "        ax7.scatter(X_pca_full[:, 0], X_pca_full[:, 1], alpha=0.3, s=30, color='gray')\n",
        "    \n",
        "    if feature_names is not None:\n",
        "        # Dibujar vectores de variables (solo las m√°s importantes)\n",
        "        scale = 4\n",
        "        top_features = np.argsort(np.abs(pca_full.components_[0]))[-8:]\n",
        "        for i in top_features:\n",
        "            ax7.arrow(0, 0, \n",
        "                     pca_full.components_[0, i]*scale, \n",
        "                     pca_full.components_[1, i]*scale,\n",
        "                     head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.6, linewidth=2)\n",
        "            ax7.text(pca_full.components_[0, i]*scale*1.15, \n",
        "                    pca_full.components_[1, i]*scale*1.15,\n",
        "                    feature_names[i], fontsize=9, ha='center', \n",
        "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
        "    \n",
        "    ax7.set_xlabel(f'PC1 ({explained_variance[0]:.1%})', fontsize=12)\n",
        "    ax7.set_ylabel(f'PC2 ({explained_variance[1]:.1%})', fontsize=12)\n",
        "    ax7.set_title('Biplot (Observaciones + Variables)', fontweight='bold', fontsize=13)\n",
        "    ax7.axhline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "    ax7.axvline(0, color='k', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "    ax7.grid(alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('An√°lisis Completo de Componentes Principales (PCA)', \n",
        "                fontsize=18, fontweight='bold', y=0.998)\n",
        "    \n",
        "    return pca_full, X_pca_full, fig\n",
        "\n",
        "# Aplicar PCA\n",
        "pca_model, X_cancer_pca, fig_pca = perform_pca_analysis(\n",
        "    X_cancer_scaled, \n",
        "    y_cancer, \n",
        "    X_cancer.columns\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 M√©todos Wrapper\n",
        "\n",
        "Eval√∫an subconjuntos de features entrenando modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_pca_3d_interactive(X_pca, y=None, explained_variance=None):\n",
        "    \"\"\"\n",
        "    Crea visualizaci√≥n 3D interactiva de PCA\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "    \n",
        "    if y is not None:\n",
        "        # Colores por clase\n",
        "        colors = ['red' if label == 0 else 'green' for label in y]\n",
        "        labels = ['Maligno' if label == 0 else 'Benigno' for label in y]\n",
        "        \n",
        "        for class_label in np.unique(y):\n",
        "            mask = y == class_label\n",
        "            class_name = 'Maligno' if class_label == 0 else 'Benigno'\n",
        "            color = 'red' if class_label == 0 else 'green'\n",
        "            \n",
        "            fig.add_trace(go.Scatter3d(\n",
        "                x=X_pca[mask, 0],\n",
        "                y=X_pca[mask, 1],\n",
        "                z=X_pca[mask, 2],\n",
        "                mode='markers',\n",
        "                name=class_name,\n",
        "                marker=dict(\n",
        "                    size=5,\n",
        "                    color=color,\n",
        "                    opacity=0.6,\n",
        "                    line=dict(color='black', width=0.5)\n",
        "                ),\n",
        "                text=[class_name] * mask.sum(),\n",
        "                hovertemplate='<b>%{text}</b><br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<br>PC3: %{z:.2f}<extra></extra>'\n",
        "            ))\n",
        "    else:\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=X_pca[:, 0],\n",
        "            y=X_pca[:, 1],\n",
        "            z=X_pca[:, 2],\n",
        "            mode='markers',\n",
        "            marker=dict(size=5, color='steelblue', opacity=0.6),\n",
        "        ))\n",
        "    \n",
        "    # Etiquetas de ejes\n",
        "    if explained_variance is not None:\n",
        "        xlabel = f'PC1 ({explained_variance[0]:.1%})'\n",
        "        ylabel = f'PC2 ({explained_variance[1]:.1%})'\n",
        "        zlabel = f'PC3 ({explained_variance[2]:.1%})'\n",
        "        total_var = explained_variance[0] + explained_variance[1] + explained_variance[2]\n",
        "        title = f'PCA 3D - Varianza Total: {total_var:.1%}'\n",
        "    else:\n",
        "        xlabel, ylabel, zlabel = 'PC1', 'PC2', 'PC3'\n",
        "        title = 'PCA 3D'\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title=dict(text=title, font=dict(size=20, color='black'), x=0.5, xanchor='center'),\n",
        "        scene=dict(\n",
        "            xaxis=dict(title=xlabel, backgroundcolor='rgb(230, 230,230)'),\n",
        "            yaxis=dict(title=ylabel, backgroundcolor='rgb(230, 230,230)'),\n",
        "            zaxis=dict(title=zlabel, backgroundcolor='rgb(230, 230,230)'),\n",
        "        ),\n",
        "        width=900,\n",
        "        height=700,\n",
        "        showlegend=True\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Crear visualizaci√≥n 3D\n",
        "fig_3d = plot_pca_3d_interactive(\n",
        "    X_cancer_pca, \n",
        "    y_cancer, \n",
        "    pca_model.explained_variance_ratio_\n",
        ")\n",
        "fig_3d.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 t-SNE para Visualizaci√≥n No Lineal\n",
        "\n",
        "t-SNE (t-Distributed Stochastic Neighbor Embedding) preserva la estructura local de los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Parte 4: Balanceo de Clases\n",
        "\n",
        "Manejaremos el desbalance de clases usando t√©cnicas de over/undersampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Creaci√≥n de Dataset Desbalanceado\n",
        "\n",
        "Simularemos un escenario realista de desbalance severo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 M√©todos Filter\n",
        "\n",
        "Eval√∫an la relevancia de cada atributo independientemente del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_filter_methods(X, y, k=15):\n",
        "    \"\"\"\n",
        "    Aplica m√∫ltiples m√©todos filter para selecci√≥n de atributos\n",
        "    \"\"\"\n",
        "    feature_names = X.columns if hasattr(X, 'columns') else [f'F{i}' for i in range(X.shape[1])]\n",
        "    results = {}\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"M√âTODOS FILTER - SELECCI√ìN DE ATRIBUTOS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # 1. ANOVA F-test (para clasificaci√≥n)\n",
        "    print(\"\\n1. Ejecutando ANOVA F-test...\", end=' ')\n",
        "    f_selector = SelectKBest(f_classif, k='all')\n",
        "    f_selector.fit(X, y)\n",
        "    results['F-test'] = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Score': f_selector.scores_,\n",
        "        'p-value': f_selector.pvalues_\n",
        "    }).sort_values('Score', ascending=False)\n",
        "    print(\"‚úì\")\n",
        "    \n",
        "    # 2. Mutual Information\n",
        "    print(\"2. Ejecutando Mutual Information...\", end=' ')\n",
        "    mi_selector = SelectKBest(mutual_info_classif, k='all')\n",
        "    mi_selector.fit(X, y)\n",
        "    results['Mutual Info'] = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Score': mi_selector.scores_\n",
        "    }).sort_values('Score', ascending=False)\n",
        "    print(\"‚úì\")\n",
        "    \n",
        "    # 3. Chi-squared (requiere valores no negativos)\n",
        "    print(\"3. Ejecutando Chi-squared...\", end=' ')\n",
        "    # Normalizar a [0, 1] para chi2\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    X_normalized = scaler.fit_transform(X)\n",
        "    chi2_selector = SelectKBest(chi2, k='all')\n",
        "    chi2_selector.fit(X_normalized, y)\n",
        "    results['Chi-squared'] = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Score': chi2_selector.scores_\n",
        "    }).sort_values('Score', ascending=False)\n",
        "    print(\"‚úì\")\n",
        "    \n",
        "    # Visualizaci√≥n\n",
        "    fig = plt.figure(figsize=(20, 6))\n",
        "    \n",
        "    for idx, (method_name, scores_df) in enumerate(results.items(), 1):\n",
        "        ax = plt.subplot(1, 3, idx)\n",
        "        top_features = scores_df.head(k)\n",
        "        \n",
        "        # Colores basados en score normalizado\n",
        "        scores_norm = (top_features['Score'] - top_features['Score'].min()) / (top_features['Score'].max() - top_features['Score'].min())\n",
        "        colors = plt.cm.RdYlGn(scores_norm)\n",
        "        \n",
        "        bars = ax.barh(range(len(top_features)), top_features['Score'].values, color=colors, edgecolor='black')\n",
        "        ax.set_yticks(range(len(top_features)))\n",
        "        ax.set_yticklabels(top_features['Feature'].values, fontsize=10)\n",
        "        ax.invert_yaxis()\n",
        "        ax.set_xlabel('Score', fontsize=12)\n",
        "        ax.set_title(f'{method_name}\\nTop {k} Features', fontweight='bold', fontsize=14)\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "        \n",
        "        # A√±adir valores\n",
        "        for i, (bar, score) in enumerate(zip(bars, top_features['Score'].values)):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width, bar.get_y() + bar.get_height()/2,\n",
        "                   f' {score:.2f}', ha='left', va='center', fontsize=9, fontweight='bold')\n",
        "    \n",
        "    plt.suptitle('M√©todos Filter: Ranking de Features', fontsize=18, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Imprimir rankings\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TOP 10 FEATURES POR M√âTODO\")\n",
        "    print(\"=\"*80)\n",
        "    for method_name, scores_df in results.items():\n",
        "        print(f\"\\n{method_name}:\")\n",
        "        print(scores_df.head(10)[['Feature', 'Score']].to_string(index=False))\n",
        "    \n",
        "    return results, fig\n",
        "\n",
        "# Aplicar m√©todos filter\n",
        "filter_results, fig_filter = apply_filter_methods(\n",
        "    pd.DataFrame(X_cancer_scaled, columns=X_cancer.columns), \n",
        "    y_cancer, \n",
        "    k=15\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Comparaci√≥n de M√©todos de Balanceo\n",
        "\n",
        "Compararemos diferentes t√©cnicas de over/undersampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_feature_correlations(X, top_features=None, threshold=0.8):\n",
        "    \"\"\"\n",
        "    Visualiza correlaciones entre features y detecta redundancia\n",
        "    \"\"\"\n",
        "    if top_features is not None:\n",
        "        X_subset = X[top_features]\n",
        "    else:\n",
        "        X_subset = X\n",
        "    \n",
        "    # Calcular correlaciones\n",
        "    corr_matrix = X_subset.corr()\n",
        "    \n",
        "    # Encontrar pares altamente correlacionados\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                high_corr_pairs.append({\n",
        "                    'Feature1': corr_matrix.columns[i],\n",
        "                    'Feature2': corr_matrix.columns[j],\n",
        "                    'Correlation': corr_matrix.iloc[i, j]\n",
        "                })\n",
        "    \n",
        "    # Visualizaci√≥n\n",
        "    fig = plt.figure(figsize=(18, 14))\n",
        "    \n",
        "    # Heatmap completo\n",
        "    ax1 = plt.subplot(2, 1, 1)\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
        "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
        "                cmap='coolwarm', center=0, square=True,\n",
        "                linewidths=0.5, cbar_kws={'label': 'Correlaci√≥n'},\n",
        "                ax=ax1, vmin=-1, vmax=1)\n",
        "    ax1.set_title('Matriz de Correlaci√≥n entre Features', fontweight='bold', fontsize=16)\n",
        "    \n",
        "    # Distribuci√≥n de correlaciones\n",
        "    ax2 = plt.subplot(2, 2, 3)\n",
        "    corr_values = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]\n",
        "    ax2.hist(corr_values, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "    ax2.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Umbral: ¬±{threshold}')\n",
        "    ax2.axvline(-threshold, color='red', linestyle='--', linewidth=2)\n",
        "    ax2.set_xlabel('Correlaci√≥n', fontsize=12)\n",
        "    ax2.set_ylabel('Frecuencia', fontsize=12)\n",
        "    ax2.set_title('Distribuci√≥n de Correlaciones', fontweight='bold', fontsize=14)\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "    \n",
        "    # Tabla de features altamente correlacionados\n",
        "    ax3 = plt.subplot(2, 2, 4)\n",
        "    ax3.axis('tight')\n",
        "    ax3.axis('off')\n",
        "    \n",
        "    if high_corr_pairs:\n",
        "        df_high_corr = pd.DataFrame(high_corr_pairs)\n",
        "        df_high_corr = df_high_corr.sort_values('Correlation', ascending=False, key=abs)\n",
        "        \n",
        "        table_data = []\n",
        "        for _, row in df_high_corr.head(15).iterrows():\n",
        "            table_data.append([\n",
        "                row['Feature1'][:20],\n",
        "                row['Feature2'][:20],\n",
        "                f\"{row['Correlation']:.3f}\"\n",
        "            ])\n",
        "        \n",
        "        table = ax3.table(cellText=table_data,\n",
        "                         colLabels=['Feature 1', 'Feature 2', 'Corr'],\n",
        "                         cellLoc='left',\n",
        "                         loc='center',\n",
        "                         colWidths=[0.4, 0.4, 0.2])\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(9)\n",
        "        table.scale(1, 2)\n",
        "        \n",
        "        # Colorear header\n",
        "        for i in range(3):\n",
        "            table[(0, i)].set_facecolor('#40466e')\n",
        "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "        \n",
        "        ax3.set_title(f'Features Altamente Correlacionados (|r| > {threshold})\\n{len(high_corr_pairs)} pares encontrados',\n",
        "                     fontweight='bold', fontsize=14, pad=20)\n",
        "    else:\n",
        "        ax3.text(0.5, 0.5, f'No hay features con |r| > {threshold}',\n",
        "                ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(f\"AN√ÅLISIS DE CORRELACIONES (umbral = {threshold})\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nTotal de pares altamente correlacionados: {len(high_corr_pairs)}\")\n",
        "    if high_corr_pairs:\n",
        "        print(\"\\nTop 10 pares m√°s correlacionados:\")\n",
        "        df_high_corr = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False, key=abs)\n",
        "        print(df_high_corr.head(10).to_string(index=False))\n",
        "    \n",
        "    return fig, high_corr_pairs\n",
        "\n",
        "# Analizar correlaciones en top features de F-test\n",
        "top_15_features = filter_results['F-test'].head(15)['Feature'].tolist()\n",
        "fig_corr, high_corr = plot_feature_correlations(\n",
        "    pd.DataFrame(X_cancer_scaled, columns=X_cancer.columns),\n",
        "    top_features=top_15_features,\n",
        "    threshold=0.8\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Visualizaci√≥n del Impacto del Balanceo\n",
        "\n",
        "Ver c√≥mo cada m√©todo afecta el espacio de features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_wrapper_methods(X, y, n_features_to_select=10):\n",
        "    \"\"\"\n",
        "    Aplica RFE (Recursive Feature Elimination) con diferentes modelos\n",
        "    \"\"\"\n",
        "    feature_names = X.columns if hasattr(X, 'columns') else [f'F{i}' for i in range(X.shape[1])]\n",
        "    \n",
        "    # Definir modelos\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"M√âTODOS WRAPPER - RFE (Recursive Feature Elimination)\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nSeleccionando top {n_features_to_select} features con cada modelo...\")\n",
        "    \n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n{model_name}...\", end=' ')\n",
        "        \n",
        "        # RFE\n",
        "        rfe = RFE(estimator=model, n_features_to_select=n_features_to_select, step=1)\n",
        "        rfe.fit(X, y)\n",
        "        \n",
        "        # Guardar resultados\n",
        "        results[model_name] = {\n",
        "            'selected': feature_names[rfe.support_].tolist(),\n",
        "            'ranking': rfe.ranking_\n",
        "        }\n",
        "        \n",
        "        print(\"‚úì\")\n",
        "        print(f\"  Features seleccionados: {results[model_name]['selected'][:5]}...\")\n",
        "    \n",
        "    # Visualizaci√≥n\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    \n",
        "    # 1. Ranking por modelo\n",
        "    for idx, (model_name, result) in enumerate(results.items(), 1):\n",
        "        ax = plt.subplot(2, 3, idx)\n",
        "        \n",
        "        ranking_df = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Ranking': result['ranking']\n",
        "        }).sort_values('Ranking')\n",
        "        \n",
        "        top_features = ranking_df.head(15)\n",
        "        colors = ['green' if r == 1 else 'orange' if r <= 3 else 'red' \n",
        "                 for r in top_features['Ranking']]\n",
        "        \n",
        "        bars = ax.barh(range(len(top_features)), top_features['Ranking'].values, \n",
        "                      color=colors, alpha=0.7, edgecolor='black')\n",
        "        ax.set_yticks(range(len(top_features)))\n",
        "        ax.set_yticklabels(top_features['Feature'].values, fontsize=9)\n",
        "        ax.invert_yaxis()\n",
        "        ax.set_xlabel('Ranking (1 = mejor)', fontsize=11)\n",
        "        ax.set_title(f'{model_name}\\nTop 15 Features', fontweight='bold', fontsize=13)\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "        \n",
        "        # A√±adir l√≠nea en ranking = n_features_to_select\n",
        "        ax.axvline(n_features_to_select, color='blue', linestyle='--', \n",
        "                  linewidth=2, alpha=0.5, label=f'Top {n_features_to_select}')\n",
        "        ax.legend()\n",
        "    \n",
        "    # 2. Diagrama de Venn (consenso)\n",
        "    ax4 = plt.subplot(2, 3, 4)\n",
        "    ax4.axis('off')\n",
        "    \n",
        "    selected_sets = {name: set(result['selected']) for name, result in results.items()}\n",
        "    \n",
        "    # Intersecciones\n",
        "    all_three = selected_sets['Logistic Regression'] & selected_sets['Random Forest'] & selected_sets['Gradient Boosting']\n",
        "    lr_rf = (selected_sets['Logistic Regression'] & selected_sets['Random Forest']) - all_three\n",
        "    lr_gb = (selected_sets['Logistic Regression'] & selected_sets['Gradient Boosting']) - all_three\n",
        "    rf_gb = (selected_sets['Random Forest'] & selected_sets['Gradient Boosting']) - all_three\n",
        "    \n",
        "    only_lr = selected_sets['Logistic Regression'] - selected_sets['Random Forest'] - selected_sets['Gradient Boosting']\n",
        "    only_rf = selected_sets['Random Forest'] - selected_sets['Logistic Regression'] - selected_sets['Gradient Boosting']\n",
        "    only_gb = selected_sets['Gradient Boosting'] - selected_sets['Logistic Regression'] - selected_sets['Random Forest']\n",
        "    \n",
        "    # Texto\n",
        "    y_pos = 0.9\n",
        "    ax4.text(0.5, y_pos, 'CONSENSO ENTRE MODELOS', ha='center', fontsize=16, fontweight='bold')\n",
        "    y_pos -= 0.1\n",
        "    \n",
        "    ax4.text(0.1, y_pos, f'üü¢ Los 3 modelos ({len(all_three)}):', fontsize=12, fontweight='bold')\n",
        "    y_pos -= 0.05\n",
        "    for feat in sorted(all_three):\n",
        "        ax4.text(0.15, y_pos, f'‚Ä¢ {feat}', fontsize=10)\n",
        "        y_pos -= 0.04\n",
        "    \n",
        "    y_pos -= 0.03\n",
        "    ax4.text(0.1, y_pos, f'üü° 2 modelos:', fontsize=12, fontweight='bold')\n",
        "    y_pos -= 0.05\n",
        "    for feat in sorted(lr_rf | lr_gb | rf_gb):\n",
        "        ax4.text(0.15, y_pos, f'‚Ä¢ {feat}', fontsize=10)\n",
        "        y_pos -= 0.04\n",
        "        if y_pos < 0.1:\n",
        "            break\n",
        "    \n",
        "    ax4.set_xlim(0, 1)\n",
        "    ax4.set_ylim(0, 1)\n",
        "    \n",
        "    # 3. Heatmap de selecci√≥n\n",
        "    ax5 = plt.subplot(2, 3, 5)\n",
        "    selection_matrix = []\n",
        "    model_names_list = list(results.keys())\n",
        "    \n",
        "    for model_name in model_names_list:\n",
        "        row = [1 if feat in results[model_name]['selected'] else 0 \n",
        "               for feat in feature_names]\n",
        "        selection_matrix.append(row)\n",
        "    \n",
        "    selection_df = pd.DataFrame(selection_matrix, \n",
        "                               index=model_names_list,\n",
        "                               columns=feature_names)\n",
        "    \n",
        "    # Ordenar por n√∫mero de selecciones\n",
        "    feature_counts = selection_df.sum(axis=0)\n",
        "    selection_df = selection_df[feature_counts.sort_values(ascending=False).index]\n",
        "    \n",
        "    sns.heatmap(selection_df.iloc[:, :20], annot=True, fmt='d', cmap='RdYlGn',\n",
        "                cbar_kws={'label': 'Seleccionado'}, ax=ax5,\n",
        "                linewidths=0.5, vmin=0, vmax=1)\n",
        "    ax5.set_title('Features Seleccionados por Modelo\\n(Top 20 m√°s frecuentes)', \n",
        "                 fontweight='bold', fontsize=13)\n",
        "    ax5.set_xlabel('')\n",
        "    ax5.set_ylabel('')\n",
        "    \n",
        "    # 4. Frecuencia de selecci√≥n\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    feature_counts_sorted = feature_counts.sort_values(ascending=False).head(15)\n",
        "    colors_freq = ['green' if c == 3 else 'orange' if c == 2 else 'red' \n",
        "                   for c in feature_counts_sorted.values]\n",
        "    \n",
        "    bars = ax6.barh(range(len(feature_counts_sorted)), feature_counts_sorted.values,\n",
        "                   color=colors_freq, alpha=0.7, edgecolor='black')\n",
        "    ax6.set_yticks(range(len(feature_counts_sorted)))\n",
        "    ax6.set_yticklabels(feature_counts_sorted.index, fontsize=10)\n",
        "    ax6.invert_yaxis()\n",
        "    ax6.set_xlabel('N√∫mero de modelos que lo seleccionaron', fontsize=11)\n",
        "    ax6.set_title('Frecuencia de Selecci√≥n\\nTop 15 Features', fontweight='bold', fontsize=13)\n",
        "    ax6.set_xticks([0, 1, 2, 3])\n",
        "    ax6.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('M√©todos Wrapper: RFE con M√∫ltiples Modelos', \n",
        "                fontsize=18, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    return results, all_three, fig\n",
        "\n",
        "# Aplicar RFE\n",
        "wrapper_results, consensus_features, fig_wrapper = apply_wrapper_methods(\n",
        "    pd.DataFrame(X_cancer_scaled, columns=X_cancer.columns),\n",
        "    y_cancer,\n",
        "    n_features_to_select=10\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURES CON CONSENSO (seleccionados por los 3 modelos):\")\n",
        "print(\"=\"*80)\n",
        "for feat in sorted(consensus_features):\n",
        "    print(f\"  ‚úì {feat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Resumen y Conclusiones\n",
        "\n",
        "## ‚úÖ Lo que hemos aprendido\n",
        "\n",
        "### 1. Limpieza de Datos\n",
        "* Los valores faltantes requieren an√°lisis cuidadoso (MCAR, MAR, MNAR)\n",
        "* KNN Imputation generalmente supera a m√©todos simples\n",
        "* Los outliers deben investigarse antes de eliminarlos\n",
        "* El escalamiento es crucial para muchos algoritmos\n",
        "\n",
        "### 2. Reducci√≥n de Dimensionalidad\n",
        "* **PCA**: R√°pido, interpretable, lineal\n",
        "  * √ötil para reducci√≥n real de dimensionalidad\n",
        "  * Preserva varianza global\n",
        "  \n",
        "* **t-SNE**: Lento, no interpretable, no lineal\n",
        "  * Excelente para visualizaci√≥n\n",
        "  * Preserva estructura local (clusters)\n",
        "\n",
        "### 3. Selecci√≥n de Atributos\n",
        "* **M√©todos Filter**: R√°pidos pero independientes del modelo\n",
        "* **M√©todos Wrapper**: M√°s lentos pero espec√≠ficos del modelo\n",
        "* **Consenso**: Combinar m√∫ltiples m√©todos aumenta robustez\n",
        "\n",
        "### 4. Balanceo de Clases\n",
        "* El desbalance severo sesga modelos hacia la mayor√≠a\n",
        "* **SMOTE** genera ejemplos sint√©ticos interpolando\n",
        "* **ADASYN** adapta la s√≠ntesis a la densidad local\n",
        "* **BorderlineSMOTE** enfoca en ejemplos frontera\n",
        "* **Random Oversampling** duplica ejemplos minoritarios\n",
        "\n",
        "## üéØ Mejores Pr√°cticas\n",
        "\n",
        "1. **Entender los datos** antes de limpiarlos\n",
        "2. **Documentar decisiones** de preprocesamiento\n",
        "3. **Validar el impacto** de cada transformaci√≥n\n",
        "4. **No eliminar datos sin investigar** primero\n",
        "5. **Escalar antes de PCA** o m√©todos basados en distancia\n",
        "6. **El balanceo es una decisi√≥n importante** seg√∫n el problema\n",
        "\n",
        "## üìä Resultados Clave de Este Notebook\n",
        "\n",
        "De nuestros experimentos:\n",
        "* El escalamiento normaliz√≥ las escalas entre variables\n",
        "* PCA redujo dimensiones preservando estructura\n",
        "* Los m√©todos Filter y Wrapper identificaron features consistentes\n",
        "* SMOTE y variantes mejoraron la representaci√≥n de clase minoritaria\n",
        "\n",
        "## üöÄ Pr√≥ximos Pasos\n",
        "\n",
        "En los siguientes notebooks veremos:\n",
        "* Integraci√≥n de t√©cnicas en pipelines completos\n",
        "* Evaluaci√≥n de modelos con m√©tricas apropiadas\n",
        "* Validaci√≥n cruzada y ajuste de hiperpar√°metros\n",
        "* Aplicaci√≥n a problemas reales de clasificaci√≥n\n",
        "\n",
        "## üìö Referencias y Recursos\n",
        "\n",
        "* Scikit-learn Documentation: https://scikit-learn.org\n",
        "* Imbalanced-learn: https://imbalanced-learn.org\n",
        "* \"Feature Engineering and Selection\" - Kuhn & Johnson\n",
        "* \"Hands-On Machine Learning\" - Aur√©lien G√©ron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üí° Ejercicios Adicionales (Opcional)\n",
        "\n",
        "Pon a prueba tu comprensi√≥n:\n",
        "\n",
        "### Ejercicio 1: Dataset Diferente\n",
        "Aplica las t√©cnicas de limpieza a otro dataset:\n",
        "* Wine Quality\n",
        "* Iris\n",
        "* Digits\n",
        "\n",
        "### Ejercicio 2: Par√°metros\n",
        "Experimenta cambiando:\n",
        "* N√∫mero de vecinos en KNN Imputer\n",
        "* Percentiles para detecci√≥n de outliers\n",
        "* N√∫mero de componentes en PCA\n",
        "* Perplexity en t-SNE\n",
        "\n",
        "### Ejercicio 3: An√°lisis Comparativo\n",
        "Compara:\n",
        "* Diferentes estrategias de imputaci√≥n en el mismo dataset\n",
        "* PCA vs selecci√≥n de features para reducci√≥n de dimensionalidad\n",
        "* Diferentes m√©todos de balanceo en m√©tricas espec√≠ficas\n",
        "\n",
        "### Ejercicio 4: Crear Dataset Propio\n",
        "Genera un dataset sint√©tico con:\n",
        "* Patrones espec√≠ficos de valores faltantes\n",
        "* Outliers controlados\n",
        "* Desbalance definido\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Excelente trabajo completando este m√≥dulo!** üéâ\n",
        "\n",
        "Has aprendido las t√©cnicas fundamentales de limpieza y preparaci√≥n que son la base para todo proyecto de ML."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
