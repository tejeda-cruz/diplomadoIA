{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/husseinlopez/diplomadoIA/blob/main/M1-6_Ejercicios_Validacion.ipynb)\n",
        "\n",
        "# M√≥dulo 1: Introducci√≥n a la Miner√≠a de Datos\n",
        "## Validaci√≥n y Evaluaci√≥n de Modelos\n",
        "\n",
        "**Diplomado en Inteligencia Artificial**  \n",
        "Dr. Irvin Hussein L√≥pez Nava\n",
        "CICESE - UABC\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objetivos de esta sesi√≥n\n",
        "\n",
        "1. **Comprender la importancia de la partici√≥n de datos** para evaluar generalizaci√≥n\n",
        "2. **Implementar diferentes esquemas de validaci√≥n**: Hold-out, k-Fold CV, Leave-One-Out\n",
        "3. **Aplicar m√©tricas de regresi√≥n**: MAE, MSE, RMSE, R¬≤\n",
        "4. **Aplicar m√©tricas de clasificaci√≥n**: Accuracy, Precision, Recall, F1, ROC-AUC, MCC\n",
        "5. **Evitar data leakage** en el pipeline de evaluaci√≥n\n",
        "6. **Interpretar resultados** considerando sesgo y varianza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estructura del notebook\n",
        "\n",
        "### Parte 1: Esquemas de Partici√≥n y Validaci√≥n\n",
        "* Hold-out simple\n",
        "* k-Fold Cross-Validation\n",
        "* Leave-One-Out Cross-Validation\n",
        "* Estratificaci√≥n\n",
        "\n",
        "### Parte 2: Evaluaci√≥n de Modelos de Regresi√≥n\n",
        "* M√©tricas: MAE, MSE, RMSE, R¬≤\n",
        "* Comparaci√≥n de modelos\n",
        "* An√°lisis de residuos\n",
        "\n",
        "### Parte 3: Evaluaci√≥n de Modelos de Clasificaci√≥n\n",
        "* Matriz de confusi√≥n\n",
        "* M√©tricas: Accuracy, Precision, Recall, F1\n",
        "* Curva ROC y AUC\n",
        "* Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "### Parte 4: Comparaci√≥n de Modelos y Mejores Pr√°cticas\n",
        "* Variabilidad del desempe√±o\n",
        "* Data leakage (c√≥mo evitarlo)\n",
        "* Selecci√≥n de m√©tricas seg√∫n contexto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0. Configuraci√≥n del Entorno\n",
        "\n",
        "Importaremos las bibliotecas necesarias para validaci√≥n y evaluaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manejo de datos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Visualizaci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Reproducibilidad\n",
        "np.random.seed(42)\n",
        "\n",
        "# Ignorar warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úì Bibliotecas b√°sicas importadas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Machine Learning\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, \n",
        "    cross_val_score, \n",
        "    cross_validate,\n",
        "    KFold, \n",
        "    StratifiedKFold, \n",
        "    LeaveOneOut\n",
        ")\n",
        "\n",
        "# Modelos\n",
        "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.svm import SVR, SVC\n",
        "\n",
        "# Preprocesamiento\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# M√©tricas de regresi√≥n\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    r2_score\n",
        ")\n",
        "\n",
        "# M√©tricas de clasificaci√≥n\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_curve,\n",
        "    roc_auc_score,\n",
        "    matthews_corrcoef\n",
        ")\n",
        "\n",
        "# Datasets\n",
        "from sklearn.datasets import (\n",
        "    load_diabetes,\n",
        "    load_breast_cancer,\n",
        "    make_classification,\n",
        "    make_regression\n",
        ")\n",
        "\n",
        "print(\"‚úì Bibliotecas de ML importadas correctamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Parte 1: Esquemas de Partici√≥n y Validaci√≥n\n",
        "\n",
        "La partici√≥n de datos es fundamental para estimar la capacidad de generalizaci√≥n del modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 ¬øPor qu√© particionar los datos?\n",
        "\n",
        "**Problema fundamental**:\n",
        "- El modelo se ajusta minimizando el error en los datos de entrenamiento\n",
        "- Si evaluamos en los mismos datos, el error ser√° **optimista**\n",
        "- No sabremos si el modelo **generaliza** a datos nuevos\n",
        "\n",
        "**Soluci√≥n**:\n",
        "- Separar datos en **entrenamiento** y **prueba**\n",
        "- Entrenar solo con training\n",
        "- Evaluar solo con test (datos \"no vistos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Hold-Out Simple\n",
        "\n",
        "La estrategia m√°s b√°sica: una sola partici√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear dataset sint√©tico para regresi√≥n\n",
        "np.random.seed(42)\n",
        "X_reg, y_reg = make_regression(\n",
        "    n_samples=200, \n",
        "    n_features=10, \n",
        "    n_informative=8,\n",
        "    noise=10, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HOLD-OUT SIMPLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Partici√≥n 70-30\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_reg, y_reg, \n",
        "    test_size=0.3, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal de datos: {len(X_reg)}\")\n",
        "print(f\"  Training: {len(X_train)} ({100*len(X_train)/len(X_reg):.0f}%)\")\n",
        "print(f\"  Test: {len(X_test)} ({100*len(X_test)/len(X_reg):.0f}%)\")\n",
        "\n",
        "# Entrenar modelo simple\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predecir\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Calcular errores\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\nResultados:\")\n",
        "print(f\"  MSE Training: {train_mse:.2f}\")\n",
        "print(f\"  MSE Test: {test_mse:.2f}\")\n",
        "print(f\"  R¬≤ Training: {train_r2:.3f}\")\n",
        "print(f\"  R¬≤ Test: {test_r2:.3f}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Limitaci√≥n: El resultado depende de la partici√≥n espec√≠fica\")\n",
        "print(f\"   Cambiar random_state da resultados diferentes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Variabilidad del Hold-Out\n",
        "\n",
        "Veamos c√≥mo cambia el desempe√±o con diferentes particiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar hold-out con diferentes semillas\n",
        "results = []\n",
        "\n",
        "for seed in range(30):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_reg, y_reg, test_size=0.3, random_state=seed\n",
        "    )\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    \n",
        "    results.append({'seed': seed, 'MSE': mse, 'R¬≤': r2})\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# MSE\n",
        "ax = axes[0]\n",
        "ax.hist(df_results['MSE'], bins=15, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "ax.axvline(df_results['MSE'].mean(), color='red', linestyle='--', linewidth=2, label=f'Media: {df_results[\"MSE\"].mean():.2f}')\n",
        "ax.set_xlabel('MSE en Test')\n",
        "ax.set_ylabel('Frecuencia')\n",
        "ax.set_title('Variabilidad del MSE\\n(30 particiones diferentes)', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# R¬≤\n",
        "ax = axes[1]\n",
        "ax.hist(df_results['R¬≤'], bins=15, alpha=0.7, color='green', edgecolor='black')\n",
        "ax.axvline(df_results['R¬≤'].mean(), color='red', linestyle='--', linewidth=2, label=f'Media: {df_results[\"R¬≤\"].mean():.3f}')\n",
        "ax.set_xlabel('R¬≤ en Test')\n",
        "ax.set_ylabel('Frecuencia')\n",
        "ax.set_title('Variabilidad del R¬≤\\n(30 particiones diferentes)', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"MSE: Œº = {df_results['MSE'].mean():.2f}, œÉ = {df_results['MSE'].std():.2f}\")\n",
        "print(f\"R¬≤:  Œº = {df_results['R¬≤'].mean():.3f}, œÉ = {df_results['R¬≤'].std():.3f}\")\n",
        "print(f\"\\nüí° Varianza alta ‚Üí Hold-out simple no es confiable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 k-Fold Cross-Validation\n",
        "\n",
        "Reduce la varianza dividiendo los datos en k subconjuntos (folds)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# k-Fold CV con k=5\n",
        "print(\"=\"*80)\n",
        "print(\"K-FOLD CROSS-VALIDATION (k=5)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "# cross_validate devuelve m√∫ltiples m√©tricas\n",
        "cv_results = cross_validate(\n",
        "    model, X_reg, y_reg,\n",
        "    cv=kfold,\n",
        "    scoring=['neg_mean_squared_error', 'r2'],\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Convertir a positivo (sklearn usa negative MSE)\n",
        "train_mse = -cv_results['train_neg_mean_squared_error']\n",
        "test_mse = -cv_results['test_neg_mean_squared_error']\n",
        "train_r2 = cv_results['train_r2']\n",
        "test_r2 = cv_results['test_r2']\n",
        "\n",
        "print(f\"\\nResultados por fold:\")\n",
        "print(f\"{'Fold':<10} {'Train MSE':<15} {'Test MSE':<15} {'Train R¬≤':<15} {'Test R¬≤':<15}\")\n",
        "print(\"-\" * 70)\n",
        "for i in range(5):\n",
        "    print(f\"{i+1:<10} {train_mse[i]:<15.2f} {test_mse[i]:<15.2f} {train_r2[i]:<15.3f} {test_r2[i]:<15.3f}\")\n",
        "\n",
        "print(\"\\nEstad√≠sticas agregadas:\")\n",
        "print(f\"  Test MSE: {test_mse.mean():.2f} ¬± {test_mse.std():.2f}\")\n",
        "print(f\"  Test R¬≤:  {test_r2.mean():.3f} ¬± {test_r2.std():.3f}\")\n",
        "\n",
        "print(f\"\\n‚úì Cada observaci√≥n se usa para entrenamiento y prueba\")\n",
        "print(f\"‚úì Estimaci√≥n m√°s estable que hold-out simple\")\n",
        "\n",
        "# Visualizaci√≥n\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# MSE por fold\n",
        "ax = axes[0]\n",
        "x = np.arange(1, 6)\n",
        "ax.bar(x - 0.2, train_mse, width=0.4, label='Training', alpha=0.7, color='steelblue')\n",
        "ax.bar(x + 0.2, test_mse, width=0.4, label='Test', alpha=0.7, color='orange')\n",
        "ax.axhline(test_mse.mean(), color='red', linestyle='--', linewidth=2, label=f'Test Œº={test_mse.mean():.2f}')\n",
        "ax.set_xlabel('Fold')\n",
        "ax.set_ylabel('MSE')\n",
        "ax.set_title('MSE por Fold (5-Fold CV)', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "# R¬≤ por fold\n",
        "ax = axes[1]\n",
        "ax.bar(x - 0.2, train_r2, width=0.4, label='Training', alpha=0.7, color='steelblue')\n",
        "ax.bar(x + 0.2, test_r2, width=0.4, label='Test', alpha=0.7, color='orange')\n",
        "ax.axhline(test_r2.mean(), color='red', linestyle='--', linewidth=2, label=f'Test Œº={test_r2.mean():.3f}')\n",
        "ax.set_xlabel('Fold')\n",
        "ax.set_ylabel('R¬≤')\n",
        "ax.set_title('R¬≤ por Fold (5-Fold CV)', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Leave-One-Out Cross-Validation (LOOCV)\n",
        "\n",
        "Caso extremo donde k = n (cada observaci√≥n es un fold)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOOCV (solo con subset peque√±o por costo computacional)\n",
        "print(\"=\"*80)\n",
        "print(\"LEAVE-ONE-OUT CROSS-VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Usar solo primeras 50 observaciones\n",
        "X_small = X_reg[:50]\n",
        "y_small = y_reg[:50]\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "# LOOCV\n",
        "scores = cross_val_score(model, X_small, y_small, cv=loo, scoring='r2')\n",
        "\n",
        "print(f\"\\nDatos: {len(X_small)} observaciones\")\n",
        "print(f\"Iteraciones: {loo.get_n_splits(X_small)} (una por observaci√≥n)\")\n",
        "print(f\"\\nR¬≤ por observaci√≥n:\")\n",
        "print(f\"  Media: {scores.mean():.3f}\")\n",
        "print(f\"  Std: {scores.std():.3f}\")\n",
        "print(f\"  Min: {scores.min():.3f}\")\n",
        "print(f\"  Max: {scores.max():.3f}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  LOOCV:\")\n",
        "print(f\"   ‚úì Bajo sesgo (entrena con n-1 datos)\")\n",
        "print(f\"   ‚úó Alta varianza\")\n",
        "print(f\"   ‚úó Alto costo computacional (n iteraciones)\")\n",
        "print(f\"   ‚Üí Usar solo con datasets peque√±os\")\n",
        "\n",
        "# Visualizaci√≥n\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "ax.hist(scores, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
        "ax.axvline(scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Media: {scores.mean():.3f}')\n",
        "ax.set_xlabel('R¬≤ Score')\n",
        "ax.set_ylabel('Frecuencia')\n",
        "ax.set_title(f'Distribuci√≥n de R¬≤ en LOOCV\\n({len(scores)} iteraciones)', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Estratificaci√≥n en Clasificaci√≥n\n",
        "\n",
        "Preservar proporciones de clase en cada fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear dataset desbalanceado para clasificaci√≥n\n",
        "X_class, y_class = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=10,\n",
        "    n_informative=8,\n",
        "    n_classes=2,\n",
        "    weights=[0.7, 0.3],  # 70% clase 0, 30% clase 1\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ESTRATIFICACI√ìN EN CLASIFICACI√ìN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nDistribuci√≥n original:\")\n",
        "unique, counts = np.unique(y_class, return_counts=True)\n",
        "for cls, count in zip(unique, counts):\n",
        "    print(f\"  Clase {cls}: {count} ({100*count/len(y_class):.1f}%)\")\n",
        "\n",
        "# CV SIN estratificaci√≥n\n",
        "print(f\"\\n--- K-Fold CV SIN estratificaci√≥n ---\")\n",
        "kfold_no_strat = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for i, (train_idx, test_idx) in enumerate(kfold_no_strat.split(X_class), 1):\n",
        "    y_test_fold = y_class[test_idx]\n",
        "    class_1_pct = 100 * (y_test_fold == 1).sum() / len(y_test_fold)\n",
        "    print(f\"  Fold {i}: {class_1_pct:.1f}% clase 1 en test\")\n",
        "\n",
        "# CV CON estratificaci√≥n\n",
        "print(f\"\\n--- K-Fold CV CON estratificaci√≥n ---\")\n",
        "kfold_strat = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for i, (train_idx, test_idx) in enumerate(kfold_strat.split(X_class, y_class), 1):\n",
        "    y_test_fold = y_class[test_idx]\n",
        "    class_1_pct = 100 * (y_test_fold == 1).sum() / len(y_test_fold)\n",
        "    print(f\"  Fold {i}: {class_1_pct:.1f}% clase 1 en test\")\n",
        "\n",
        "print(f\"\\n‚úì Estratificaci√≥n preserva las proporciones originales\")\n",
        "print(f\"‚úì Esencial con clases desbalanceadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Parte 2: Evaluaci√≥n de Modelos de Regresi√≥n\n",
        "\n",
        "M√©tricas para problemas donde la variable objetivo es continua."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Dataset: Diabetes\n",
        "\n",
        "Usaremos el dataset cl√°sico de diabetes para predecir progresi√≥n de la enfermedad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset\n",
        "diabetes = load_diabetes()\n",
        "X_diabetes = diabetes.data\n",
        "y_diabetes = diabetes.target\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATASET: Diabetes\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nObservaciones: {X_diabetes.shape[0]}\")\n",
        "print(f\"Features: {X_diabetes.shape[1]}\")\n",
        "print(f\"\\nVariable objetivo: Progresi√≥n de diabetes (continua)\")\n",
        "print(f\"  Min: {y_diabetes.min():.1f}\")\n",
        "print(f\"  Max: {y_diabetes.max():.1f}\")\n",
        "print(f\"  Media: {y_diabetes.mean():.1f}\")\n",
        "print(f\"  Std: {y_diabetes.std():.1f}\")\n",
        "\n",
        "# Partici√≥n\n",
        "X_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n",
        "    X_diabetes, y_diabetes, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nPartici√≥n 70-30:\")\n",
        "print(f\"  Training: {len(X_train_diab)}\")\n",
        "print(f\"  Test: {len(X_test_diab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 M√©tricas de Regresi√≥n\n",
        "\n",
        "Compararemos MAE, MSE, RMSE y R¬≤ en diferentes modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_regression_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Eval√∫a un modelo de regresi√≥n con m√∫ltiples m√©tricas\n",
        "    \"\"\"\n",
        "    # Entrenar\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predecir\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calcular m√©tricas\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Train MAE': mean_absolute_error(y_train, y_train_pred),\n",
        "        'Test MAE': mean_absolute_error(y_test, y_test_pred),\n",
        "        'Train MSE': mean_squared_error(y_train, y_train_pred),\n",
        "        'Test MSE': mean_squared_error(y_test, y_test_pred),\n",
        "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
        "        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
        "        'Train R¬≤': r2_score(y_train, y_train_pred),\n",
        "        'Test R¬≤': r2_score(y_test, y_test_pred)\n",
        "    }\n",
        "    \n",
        "    return metrics, y_test_pred\n",
        "\n",
        "# Modelos a comparar\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge (Œ±=1.0)': Ridge(alpha=1.0),\n",
        "    'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42),\n",
        "    'KNN (k=5)': KNeighborsRegressor(n_neighbors=5)\n",
        "}\n",
        "\n",
        "results = []\n",
        "predictions = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPARACI√ìN DE MODELOS - REGRESI√ìN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for name, model in models.items():\n",
        "    metrics, y_pred = evaluate_regression_model(\n",
        "        model, X_train_diab, X_test_diab, y_train_diab, y_test_diab, name\n",
        "    )\n",
        "    results.append(metrics)\n",
        "    predictions[name] = y_pred\n",
        "    print(f\"‚úì {name}\")\n",
        "\n",
        "df_results_reg = pd.DataFrame(results)\n",
        "print(f\"\\n{df_results_reg.to_string(index=False)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizaci√≥n de Resultados\n",
        "\n",
        "Comparemos las m√©tricas y las predicciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de m√©tricas\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "metrics_to_plot = [\n",
        "    ('Test MAE', 'MAE en Test', 'steelblue'),\n",
        "    ('Test RMSE', 'RMSE en Test', 'orange'),\n",
        "    ('Test R¬≤', 'R¬≤ en Test', 'green'),\n",
        "]\n",
        "\n",
        "for idx, (metric, title, color) in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    \n",
        "    values = df_results_reg[metric].values\n",
        "    models_names = df_results_reg['Model'].values\n",
        "    \n",
        "    bars = ax.barh(models_names, values, color=color, alpha=0.7, edgecolor='black')\n",
        "    ax.set_xlabel(metric, fontsize=11)\n",
        "    ax.set_title(title, fontweight='bold', fontsize=13)\n",
        "    ax.grid(alpha=0.3, axis='x')\n",
        "    \n",
        "    # A√±adir valores\n",
        "    for bar, val in zip(bars, values):\n",
        "        width = bar.get_width()\n",
        "        ax.text(width, bar.get_y() + bar.get_height()/2,\n",
        "               f' {val:.2f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Predicciones vs Real (mejor modelo por R¬≤)\n",
        "ax = axes[1, 1]\n",
        "best_model_name = df_results_reg.loc[df_results_reg['Test R¬≤'].idxmax(), 'Model']\n",
        "y_pred_best = predictions[best_model_name]\n",
        "\n",
        "ax.scatter(y_test_diab, y_pred_best, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "ax.plot([y_test_diab.min(), y_test_diab.max()], \n",
        "        [y_test_diab.min(), y_test_diab.max()], \n",
        "        'r--', linewidth=2, label='Predicci√≥n perfecta')\n",
        "ax.set_xlabel('Valores Reales', fontsize=11)\n",
        "ax.set_ylabel('Valores Predichos', fontsize=11)\n",
        "ax.set_title(f'Predicciones vs Reales\\n(Mejor modelo: {best_model_name})', fontweight='bold', fontsize=13)\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('Comparaci√≥n de Modelos de Regresi√≥n', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüèÜ Mejor modelo por R¬≤: {best_model_name}\")\n",
        "print(f\"   Test R¬≤: {df_results_reg.loc[df_results_reg['Test R¬≤'].idxmax(), 'Test R¬≤']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 An√°lisis de Residuos\n",
        "\n",
        "Los residuos revelan patrones de error del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de residuos del mejor modelo\n",
        "best_model_idx = df_results_reg['Test R¬≤'].idxmax()\n",
        "best_model_name = df_results_reg.loc[best_model_idx, 'Model']\n",
        "y_pred_best = predictions[best_model_name]\n",
        "\n",
        "residuals = y_test_diab - y_pred_best\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Distribuci√≥n de residuos\n",
        "ax = axes[0]\n",
        "ax.hist(residuals, bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Cero')\n",
        "ax.axvline(residuals.mean(), color='orange', linestyle='--', linewidth=2, label=f'Media: {residuals.mean():.2f}')\n",
        "ax.set_xlabel('Residuos')\n",
        "ax.set_ylabel('Frecuencia')\n",
        "ax.set_title('Distribuci√≥n de Residuos', fontweight='bold')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Residuos vs Predicciones\n",
        "ax = axes[1]\n",
        "ax.scatter(y_pred_best, residuals, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)\n",
        "ax.axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "ax.set_xlabel('Valores Predichos')\n",
        "ax.set_ylabel('Residuos')\n",
        "ax.set_title('Residuos vs Predicciones', fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# Q-Q plot\n",
        "ax = axes[2]\n",
        "stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
        "ax.set_title('Q-Q Plot (normalidad de residuos)', fontweight='bold')\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle(f'An√°lisis de Residuos: {best_model_name}', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Estad√≠sticas de residuos:\")\n",
        "print(f\"  Media: {residuals.mean():.2f} (debe estar cerca de 0)\")\n",
        "print(f\"  Std: {residuals.std():.2f}\")\n",
        "print(f\"  Min: {residuals.min():.2f}\")\n",
        "print(f\"  Max: {residuals.max():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Parte 3: Evaluaci√≥n de Modelos de Clasificaci√≥n\n",
        "\n",
        "M√©tricas para problemas donde la variable objetivo es categ√≥rica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Dataset: Breast Cancer\n",
        "\n",
        "Usaremos el dataset Wisconsin Breast Cancer (clasificaci√≥n binaria)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar dataset\n",
        "cancer = load_breast_cancer()\n",
        "X_cancer = cancer.data\n",
        "y_cancer = cancer.target\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATASET: Wisconsin Breast Cancer\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nObservaciones: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "print(f\"\\nClases:\")\n",
        "unique, counts = np.unique(y_cancer, return_counts=True)\n",
        "for cls, count, name in zip(unique, counts, cancer.target_names):\n",
        "    print(f\"  {cls} ({name}): {count} ({100*count/len(y_cancer):.1f}%)\")\n",
        "\n",
        "# Partici√≥n estratificada\n",
        "X_train_canc, X_test_canc, y_train_canc, y_test_canc = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer\n",
        ")\n",
        "\n",
        "print(f\"\\nPartici√≥n estratificada 70-30:\")\n",
        "print(f\"  Training: {len(X_train_canc)}\")\n",
        "print(f\"  Test: {len(X_test_canc)}\")\n",
        "\n",
        "# Verificar estratificaci√≥n\n",
        "print(f\"\\nDistribuci√≥n en test:\")\n",
        "unique, counts = np.unique(y_test_canc, return_counts=True)\n",
        "for cls, count in zip(unique, counts):\n",
        "    print(f\"  Clase {cls}: {count} ({100*count/len(y_test_canc):.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Matriz de Confusi√≥n\n",
        "\n",
        "La base para todas las m√©tricas de clasificaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
        "    \"\"\"\n",
        "    Visualiza la matriz de confusi√≥n\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "    \n",
        "    # Heatmap\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Cuenta'}, ax=ax)\n",
        "    \n",
        "    ax.set_ylabel('Clase Real', fontsize=12)\n",
        "    ax.set_xlabel('Clase Predicha', fontsize=12)\n",
        "    ax.set_title(f'Matriz de Confusi√≥n\\n{model_name}', fontweight='bold', fontsize=14)\n",
        "    \n",
        "    # A√±adir anotaciones TN, FP, FN, TP\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    annotations = [\n",
        "        (0, 0, f'TN={tn}'),\n",
        "        (1, 0, f'FP={fp}'),\n",
        "        (0, 1, f'FN={fn}'),\n",
        "        (1, 1, f'TP={tp}')\n",
        "    ]\n",
        "    \n",
        "    for x, y, text in annotations:\n",
        "        ax.text(x + 0.5, y + 0.75, text, ha='center', va='center',\n",
        "               fontsize=10, color='darkred', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig, cm\n",
        "\n",
        "# Entrenar modelo simple\n",
        "model_lr = LogisticRegression(max_iter=10000, random_state=42)\n",
        "model_lr.fit(X_train_canc, y_train_canc)\n",
        "y_pred_lr = model_lr.predict(X_test_canc)\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "fig_cm, cm = plot_confusion_matrix(y_test_canc, y_pred_lr, cancer.target_names, 'Logistic Regression')\n",
        "plt.show()\n",
        "\n",
        "# M√©tricas b√°sicas\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"=\"*80)\n",
        "print(\"M√âTRICAS DESDE LA MATRIZ DE CONFUSI√ìN\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nVerdaderos Negativos (TN): {tn}\")\n",
        "print(f\"Falsos Positivos (FP):     {fp}  ‚Üê Error Tipo I\")\n",
        "print(f\"Falsos Negativos (FN):     {fn}  ‚Üê Error Tipo II\")\n",
        "print(f\"Verdaderos Positivos (TP): {tp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 M√©tricas de Clasificaci√≥n\n",
        "\n",
        "Calculemos todas las m√©tricas principales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_classification_metrics(y_true, y_pred, y_pred_proba=None):\n",
        "    \"\"\"\n",
        "    Calcula todas las m√©tricas de clasificaci√≥n\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred),\n",
        "        'Recall': recall_score(y_true, y_pred),\n",
        "        'F1-Score': f1_score(y_true, y_pred),\n",
        "        'MCC': matthews_corrcoef(y_true, y_pred)\n",
        "    }\n",
        "    \n",
        "    if y_pred_proba is not None:\n",
        "        metrics['ROC-AUC'] = roc_auc_score(y_true, y_pred_proba)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# M√©tricas para Logistic Regression\n",
        "y_pred_proba_lr = model_lr.predict_proba(X_test_canc)[:, 1]\n",
        "metrics_lr = calculate_classification_metrics(y_test_canc, y_pred_lr, y_pred_proba_lr)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"M√âTRICAS DE CLASIFICACI√ìN - Logistic Regression\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for metric, value in metrics_lr.items():\n",
        "    print(f\"{metric:15s}: {value:.4f}\")\n",
        "\n",
        "print(f\"\\nInterpretaci√≥n:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {metrics_lr['Accuracy']:.1%} de predicciones correctas\")\n",
        "print(f\"  ‚Ä¢ Precision: {metrics_lr['Precision']:.1%} de positivos predichos son correctos\")\n",
        "print(f\"  ‚Ä¢ Recall:    {metrics_lr['Recall']:.1%} de positivos reales fueron detectados\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  Media arm√≥nica de Precision y Recall\")\n",
        "print(f\"  ‚Ä¢ MCC:       Correlaci√≥n entre predicci√≥n y realidad [-1, 1]\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   √Årea bajo la curva ROC [0.5, 1.0]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Curva ROC y AUC\n",
        "\n",
        "La curva ROC eval√∫a el desempe√±o variando el umbral de clasificaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular curva ROC\n",
        "fpr, tpr, thresholds = roc_curve(y_test_canc, y_pred_proba_lr)\n",
        "roc_auc = roc_auc_score(y_test_canc, y_pred_proba_lr)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Curva ROC\n",
        "ax = axes[0]\n",
        "ax.plot(fpr, tpr, linewidth=3, label=f'Logistic Regression (AUC = {roc_auc:.3f})', color='steelblue')\n",
        "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Clasificador aleatorio (AUC = 0.5)', alpha=0.5)\n",
        "ax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
        "ax.set_ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
        "ax.set_title('Curva ROC', fontweight='bold', fontsize=14)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(alpha=0.3)\n",
        "ax.set_xlim([-0.05, 1.05])\n",
        "ax.set_ylim([-0.05, 1.05])\n",
        "\n",
        "# Umbral √≥ptimo\n",
        "# Criterio: maximizar TPR - FPR\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "ax.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=10, \n",
        "        label=f'Umbral √≥ptimo: {optimal_threshold:.3f}')\n",
        "ax.legend(fontsize=11)\n",
        "\n",
        "# Distribuci√≥n de probabilidades\n",
        "ax = axes[1]\n",
        "y_pred_proba_class0 = y_pred_proba_lr[y_test_canc == 0]\n",
        "y_pred_proba_class1 = y_pred_proba_lr[y_test_canc == 1]\n",
        "\n",
        "ax.hist(y_pred_proba_class0, bins=30, alpha=0.6, color='red', label='Clase 0 (Maligno)', edgecolor='black')\n",
        "ax.hist(y_pred_proba_class1, bins=30, alpha=0.6, color='green', label='Clase 1 (Benigno)', edgecolor='black')\n",
        "ax.axvline(0.5, color='black', linestyle='--', linewidth=2, label='Umbral por defecto: 0.5')\n",
        "ax.axvline(optimal_threshold, color='orange', linestyle='--', linewidth=2, label=f'Umbral √≥ptimo: {optimal_threshold:.3f}')\n",
        "ax.set_xlabel('Probabilidad predicha (clase 1)', fontsize=12)\n",
        "ax.set_ylabel('Frecuencia', fontsize=12)\n",
        "ax.set_title('Distribuci√≥n de Probabilidades Predichas', fontweight='bold', fontsize=14)\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('An√°lisis ROC y Probabilidades', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "print(f\"Interpretaci√≥n: Probabilidad de que el modelo asigne mayor score a un\")\n",
        "print(f\"                positivo real que a un negativo real\")\n",
        "print(f\"\\nUmbral √≥ptimo: {optimal_threshold:.3f} (vs 0.5 por defecto)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Comparaci√≥n de M√∫ltiples Modelos\n",
        "\n",
        "Evaluemos diferentes clasificadores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_classifier(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Eval√∫a un modelo de clasificaci√≥n con todas las m√©tricas\n",
        "    \"\"\"\n",
        "    # Entrenar\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predecir\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # M√©tricas\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred),\n",
        "        'Recall': recall_score(y_test, y_pred),\n",
        "        'F1-Score': f1_score(y_test, y_pred),\n",
        "        'MCC': matthews_corrcoef(y_test, y_pred)\n",
        "    }\n",
        "    \n",
        "    if y_pred_proba is not None:\n",
        "        metrics['ROC-AUC'] = roc_auc_score(y_test, y_pred_proba)\n",
        "    else:\n",
        "        metrics['ROC-AUC'] = np.nan\n",
        "    \n",
        "    return metrics, y_pred, y_pred_proba\n",
        "\n",
        "# Modelos a comparar\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=10000, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
        "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
        "    'SVM (linear)': SVC(kernel='linear', probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "results_class = []\n",
        "predictions_class = {}\n",
        "probas_class = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPARACI√ìN DE CLASIFICADORES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for name, model in classifiers.items():\n",
        "    metrics, y_pred, y_proba = evaluate_classifier(\n",
        "        model, X_train_canc, X_test_canc, y_train_canc, y_test_canc, name\n",
        "    )\n",
        "    results_class.append(metrics)\n",
        "    predictions_class[name] = y_pred\n",
        "    probas_class[name] = y_proba\n",
        "    print(f\"‚úì {name}\")\n",
        "\n",
        "df_results_class = pd.DataFrame(results_class)\n",
        "print(f\"\\n{df_results_class.to_string(index=False)}\")\n",
        "\n",
        "# Identificar mejor modelo\n",
        "best_f1_idx = df_results_class['F1-Score'].idxmax()\n",
        "best_model_name = df_results_class.loc[best_f1_idx, 'Model']\n",
        "print(f\"\\nüèÜ Mejor modelo por F1-Score: {best_model_name}\")\n",
        "print(f\"   F1-Score: {df_results_class.loc[best_f1_idx, 'F1-Score']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizaci√≥n Comparativa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de m√©tricas\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'MCC', 'ROC-AUC']\n",
        "colors = ['steelblue', 'orange', 'green', 'red', 'purple', 'brown']\n",
        "\n",
        "for idx, (metric, color) in enumerate(zip(metrics_names, colors)):\n",
        "    ax = axes[idx]\n",
        "    \n",
        "    values = df_results_class[metric].values\n",
        "    models = df_results_class['Model'].values\n",
        "    \n",
        "    bars = ax.barh(models, values, color=color, alpha=0.7, edgecolor='black')\n",
        "    ax.set_xlabel(metric, fontsize=11)\n",
        "    ax.set_title(metric, fontweight='bold', fontsize=13)\n",
        "    ax.grid(alpha=0.3, axis='x')\n",
        "    ax.set_xlim(0, 1.05)\n",
        "    \n",
        "    # A√±adir valores\n",
        "    for bar, val in zip(bars, values):\n",
        "        if not np.isnan(val):\n",
        "            width = bar.get_width()\n",
        "            ax.text(width + 0.02, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{val:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Comparaci√≥n de Clasificadores - Todas las M√©tricas', \n",
        "            fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Parte 4: Mejores Pr√°cticas en Validaci√≥n\n",
        "\n",
        "C√≥mo evitar errores comunes y obtener evaluaciones confiables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Data Leakage: El Enemigo Silencioso\n",
        "\n",
        "La fuga de informaci√≥n invalida la evaluaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EJEMPLO DE DATA LEAKAGE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear dataset simple\n",
        "X_leak, y_leak = make_classification(n_samples=200, n_features=10, random_state=42)\n",
        "\n",
        "print(f\"\\n--- INCORRECTO: Escalar antes de dividir ---\")\n",
        "# ‚ùå MAL: Escalar con todo el dataset\n",
        "scaler_wrong = StandardScaler()\n",
        "X_scaled_wrong = scaler_wrong.fit_transform(X_leak)\n",
        "\n",
        "# Luego dividir\n",
        "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
        "    X_scaled_wrong, y_leak, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Entrenar y evaluar\n",
        "model_wrong = LogisticRegression(max_iter=1000)\n",
        "model_wrong.fit(X_train_wrong, y_train_wrong)\n",
        "acc_wrong = model_wrong.score(X_test_wrong, y_test_wrong)\n",
        "\n",
        "print(f\"Accuracy (con leakage): {acc_wrong:.4f}\")\n",
        "print(f\"‚ö†Ô∏è  El scaler vio TODO el dataset ‚Üí informaci√≥n del test filtr√≥ al train\")\n",
        "\n",
        "print(f\"\\n--- CORRECTO: Escalar solo con training ---\")\n",
        "# ‚úÖ BIEN: Primero dividir\n",
        "X_train_right, X_test_right, y_train_right, y_test_right = train_test_split(\n",
        "    X_leak, y_leak, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Luego escalar solo con training\n",
        "scaler_right = StandardScaler()\n",
        "X_train_scaled = scaler_right.fit_transform(X_train_right)\n",
        "X_test_scaled = scaler_right.transform(X_test_right)  # Solo transform, NO fit_transform\n",
        "\n",
        "# Entrenar y evaluar\n",
        "model_right = LogisticRegression(max_iter=1000)\n",
        "model_right.fit(X_train_scaled, y_train_right)\n",
        "acc_right = model_right.score(X_test_scaled, y_test_right)\n",
        "\n",
        "print(f\"Accuracy (sin leakage): {acc_right:.4f}\")\n",
        "print(f\"‚úì El scaler solo vio training ‚Üí evaluaci√≥n v√°lida\")\n",
        "\n",
        "print(f\"\\nDiferencia: {acc_wrong - acc_right:.4f}\")\n",
        "print(f\"La evaluaci√≥n con leakage es artificialmente optimista!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Usando Pipelines para Evitar Leakage\n",
        "\n",
        "Los pipelines automatizan el flujo correcto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Pipeline correcto\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Cross-validation con pipeline\n",
        "cv_scores = cross_val_score(pipeline, X_leak, y_leak, cv=5, scoring='accuracy')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PIPELINE + CROSS-VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nAccuracy por fold:\")\n",
        "for i, score in enumerate(cv_scores, 1):\n",
        "    print(f\"  Fold {i}: {score:.4f}\")\n",
        "\n",
        "print(f\"\\nMedia: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
        "print(f\"\\n‚úì El pipeline garantiza que el escalamiento se hace DENTRO de cada fold\")\n",
        "print(f\"‚úì No hay leakage entre training y test en ning√∫n fold\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Selecci√≥n de M√©tricas Seg√∫n Contexto\n",
        "\n",
        "No existe una m√©trica universalmente superior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"GU√çA DE SELECCI√ìN DE M√âTRICAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "üìä REGRESI√ìN:\n",
        "  M√©trica    | Cu√°ndo usar                        | Sensibilidad a outliers\n",
        "  -----------|------------------------------------|-----------------------\n",
        "  MAE        | Errores en escala original         | Baja (lineal)\n",
        "  MSE        | Penalizar errores grandes          | Alta (cuadr√°tica)\n",
        "  RMSE       | MSE en escala original             | Alta (cuadr√°tica)\n",
        "  R¬≤         | Proporci√≥n de varianza explicada   | Media\n",
        "\n",
        "üéØ CLASIFICACI√ìN (Binaria):\n",
        "  Escenario                      | M√©trica prioritaria\n",
        "  -------------------------------|--------------------\n",
        "  Clases balanceadas             | Accuracy, F1-Score\n",
        "  Clases desbalanceadas          | F1, MCC, ROC-AUC\n",
        "  Minimizar falsos positivos     | Precision\n",
        "  Minimizar falsos negativos     | Recall (Sensitivity)\n",
        "  Balance precision-recall       | F1-Score\n",
        "  Todas las celdas de CM         | MCC\n",
        "  Evaluar umbral variable        | ROC-AUC\n",
        "\n",
        "‚öïÔ∏è EJEMPLOS PR√ÅCTICOS:\n",
        "  Problema                | Por qu√©                           | M√©trica\n",
        "  ------------------------|-----------------------------------|----------\n",
        "  Diagn√≥stico c√°ncer      | FN son cr√≠ticos (no detectar)     | Recall\n",
        "  Detecci√≥n fraude        | FP son costosos (falsa alarma)    | Precision\n",
        "  Spam filtering          | Balance FP y FN                   | F1-Score\n",
        "  Clases muy desbalanceadas| Accuracy enga√±osa                | MCC, F1\n",
        "  \n",
        "‚ö†Ô∏è  IMPORTANTE:\n",
        "  ‚Ä¢ Accuracy es enga√±osa con desbalance (puede ser alta prediciendo siempre la mayor√≠a)\n",
        "  ‚Ä¢ Precision y Recall son opuestos (mejora de uno empeora el otro)\n",
        "  ‚Ä¢ F1 balancea Precision y Recall\n",
        "  ‚Ä¢ MCC es robusto con cualquier distribuci√≥n de clases\n",
        "  ‚Ä¢ ROC-AUC eval√∫a todos los posibles umbrales\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Resumen y Conclusiones\n",
        "\n",
        "## ‚úÖ Lo que hemos aprendido\n",
        "\n",
        "### 1. Partici√≥n de Datos\n",
        "* **Hold-out simple**: R√°pido pero con alta varianza\n",
        "* **k-Fold CV**: Est√°ndar pr√°ctico (k=5 t√≠picamente)\n",
        "* **Leave-One-Out**: Bajo sesgo, alta varianza, costoso\n",
        "* **Estratificaci√≥n**: Esencial con clases desbalanceadas\n",
        "\n",
        "### 2. M√©tricas de Regresi√≥n\n",
        "* **MAE**: Robusto a outliers, escala original\n",
        "* **MSE**: Penaliza errores grandes, sensible a outliers\n",
        "* **RMSE**: MSE en escala original\n",
        "* **R¬≤**: Proporci√≥n de varianza explicada\n",
        "\n",
        "### 3. M√©tricas de Clasificaci√≥n\n",
        "* **Accuracy**: Simple pero enga√±osa con desbalance\n",
        "* **Precision**: De los positivos predichos, cu√°ntos son correctos\n",
        "* **Recall**: De los positivos reales, cu√°ntos detectamos\n",
        "* **F1-Score**: Balance entre Precision y Recall\n",
        "* **ROC-AUC**: Desempe√±o agregado en todos los umbrales\n",
        "* **MCC**: Robusto con cualquier distribuci√≥n de clases\n",
        "\n",
        "### 4. Mejores Pr√°cticas\n",
        "* **Evitar data leakage**: Preprocesar DENTRO de cada fold\n",
        "* **Usar pipelines**: Automatizan el flujo correcto\n",
        "* **Reportar varianza**: El desempe√±o no es un n√∫mero fijo\n",
        "* **Seleccionar m√©trica apropiada**: Seg√∫n el contexto del problema\n",
        "\n",
        "## üéØ Principios Clave\n",
        "\n",
        "1. **Sin partici√≥n no hay evaluaci√≥n v√°lida**\n",
        "2. **El error en training es optimista**\n",
        "3. **Cross-validation reduce varianza**\n",
        "4. **La m√©trica debe reflejar el objetivo del problema**\n",
        "5. **Data leakage invalida la evaluaci√≥n**\n",
        "\n",
        "## üìö Pr√≥ximos Pasos\n",
        "\n",
        "En m√≥dulos posteriores veremos:\n",
        "* Optimizaci√≥n de hiperpar√°metros\n",
        "* Nested cross-validation\n",
        "* Evaluaci√≥n de modelos complejos\n",
        "* Interpretabilidad de resultados\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Excelente trabajo!** üéâ  \n",
        "Has completado el m√≥dulo de validaci√≥n y evaluaci√≥n de modelos."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
